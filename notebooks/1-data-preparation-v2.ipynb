{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4065809-62c9-48af-b408-f4854fd1e93d",
   "metadata": {},
   "source": [
    "# Sexism Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351e9106-0564-4e29-8623-5394b274b52c",
   "metadata": {},
   "source": [
    "In any Data Science project, the first step is to obtain a dataset to work with. In the case of VAWG, data has been collected from various sources to acquire a considerable number of texts labeled as either sexist or non-sexist.\n",
    "\n",
    "This Notebook outlines the processes that were followed to obtain the final dataset and presents the implemented functions that will be included in the `utils.data` module. Data preprocessing in this case consists of three phases:\n",
    "\n",
    "1. **Source Integration**: involves the homogenization and merging of all collected datasets to create a unified dataset.\n",
    "1. **Text Cleaning**: involves the removal of all characters that could negatively affect the model.\n",
    "1. **Specific Translation**: involves translating data from another language to Spanish. In our case, the data has been translated into Mexican Spanish using GPT-4 since the project focuses on Mexico. This decision was made because GPT-4 has the ability to adjust the text to sound more like Mexican Spanish, including more culturally specific expressions. Other translation tools such as DeepTL or Google Translate were not chosen because they do not have the same level of customization as GPT-4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e149c689-c5b8-4f05-b755-04d75306d305",
   "metadata": {},
   "source": [
    "**Requiered libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b07f5c-e34b-4450-864d-b66873795a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to handle text\n",
    "import re  # regexs\n",
    "import emoji  # emojis \n",
    "import string  # punctuation\n",
    "import unidecode  # accents\n",
    "\n",
    "# to manage data\n",
    "import collections  # additional data structures\n",
    "import pandas as pd  # efficient tabular data \n",
    "import numpy as np  # efficient arrays\n",
    "\n",
    "# to translate text\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "# misc\n",
    "import os\n",
    "import time\n",
    "import scipy.stats as st\n",
    "from copy import deepcopy  # for copying deep dictionaries\n",
    "\n",
    "# support for type hints\n",
    "from typing import List, Dict, Tuple, Iterator, Union, Any, Optional "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ceeff-df87-4720-b4c1-aa6e05f5a580",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Source Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca0685b-7ade-4dc8-9767-93713c467898",
   "metadata": {},
   "source": [
    "The first step is to merge all the datasets into one while taking into account the structural differences between them. For example, not all datasets have the same variable names, some have more variables, and others may lack variables that are of interest to us. It can be quite a mess.\n",
    "\n",
    "To carry out this integration, two functions have been defined:\n",
    "\n",
    "- `homogenize`: This function is used to convert a dataset's specific format (column names, variables, etc.) to a \"common\" format (same number of columns, order, names, etc.) so that they can be easily integrated by concatenating them.\n",
    "\n",
    "- `integrate`: This function concatenates the data and has been implemented to work with an \"integration schema.\" This schema allows for the reading, homogenization, and integration of all sources at once without having to call the functions manually for each dataset.\n",
    "\n",
    "To further streamline the process, these functions have been wrapped into an object called `SexismDataIntegrator`, where they are defined as methods. By defining these functions as methods, they can be accessed through a single object, making it easier to use and manage them. Additionally, this approach allows for better encapsulation of the data integration process, making it easier to maintain and modify in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c11774a0-6cfb-41eb-9f67-7fc5ff68ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = lambda x, positive: 1 if x == positive else 0\n",
    "int2label = lambda x, positive, negative: positive if x else negative\n",
    "\n",
    "\n",
    "class SexismDataIntegrator:\n",
    "    def __init__(self, \n",
    "            common_columns: collections.OrderedDict,\n",
    "            original_data_path: str,\n",
    "            transformed_data_path: str\n",
    "        ):\n",
    "        \"\"\"\n",
    "        PARAMETERS:\n",
    "            - common_columns: must contain the key \"target\".\n",
    "            - original_data_path: ...\n",
    "            - transformed_data_path: ...\n",
    "        \"\"\"\n",
    "        self.common = common_columns\n",
    "        self.orig_data_path = original_data_path\n",
    "        self.trans_data_path = transformed_data_path\n",
    "        \n",
    "    def homogenize(self,\n",
    "                   \n",
    "            *datasets: List[pd.DataFrame],\n",
    "            column_mapping: Dict[Union[str, None], Union[str, list]] = {},\n",
    "            label_positive: Optional[Union[bool, str]] = None,\n",
    "            target_column: Optional[str] = None,\n",
    "            language: Optional[str] = None,\n",
    "            dataset_name: Optional[str] = None\n",
    "                   \n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Set a common format for any data source. Data returned will have the columns: specified in self.COMMON plus \n",
    "        \"dataset\" (if required). It also supports valuing the language column if present in self.COMMON and not in\n",
    "        original data.  \n",
    "\n",
    "        PARAMETERS: \n",
    "            - datasets: list of data frames to homogenize (same format expected between them). Starred expression\n",
    "              allow to pass directly several pandas.DataFrame objects.\n",
    "            - column_mapping: dictionary containing how to map actual to required columns if different names. It may\n",
    "              contain the key \"None\", which must map with a list containing the required common columns not present\n",
    "              in the data source. \n",
    "            - label_positive: the positive label to consider when astyping target column to int (if needed, \n",
    "              i.e. target column is not int).\n",
    "            - dataset_name: name to identify the data source.\n",
    "            - language: data source text's language (if not present in column_mapping).\n",
    "\n",
    "        RETURNS: homogenized data.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(datasets) == 1:\n",
    "            aux = datasets[0].copy()\n",
    "        else: \n",
    "            aux = pd.concat(datasets, ignore_index=True)\n",
    "                    \n",
    "        # some relevant variables\n",
    "        len_df = len(aux)\n",
    "        cmap = deepcopy(column_mapping)\n",
    "        \n",
    "        # add required columns not present in the source\n",
    "        none_columns = cmap.get(None, 0)\n",
    "        if none_columns:\n",
    "            for column in none_columns: aux[column] = [None] * len_df \n",
    "            del cmap[None]\n",
    "            \n",
    "        # renaming columns to common names\n",
    "        aux = aux.rename(columns=cmap) \n",
    "\n",
    "        # astype target column to int if needed\n",
    "        if label_positive is not None:\n",
    "\n",
    "            # if its boolean, astyping directly more efficient\n",
    "            if isinstance(label_positive, bool): \n",
    "                aux[self.common[\"target\"]] = aux[self.common[\"target\"]].astype(int)\n",
    "\n",
    "            # if its string, conventional label to int\n",
    "            elif isinstance(label_positive, str): \n",
    "                aux[self.common[\"target\"]] = aux[self.common[\"target\"]].apply(label2int, positive=label_positive)\n",
    "                \n",
    "        # add language column\n",
    "        if language is not None:\n",
    "            aux['language'] = [language] * len_df\n",
    "        \n",
    "        # select common columns\n",
    "        relevant_columns = list(self.common.values()) \n",
    "        aux = aux.loc[:, relevant_columns]\n",
    "\n",
    "        # add dataset name column \n",
    "        if dataset_name is not None:\n",
    "            aux['dataset'] = [dataset_name] * len_df\n",
    "\n",
    "        return aux\n",
    "    \n",
    "    def integration_schema_template(self):\n",
    "        print(\"\"\"\n",
    "            schema = {\n",
    "             <data_source_folder_name_in_original_data_path>: {\n",
    "                 \"read\": {\n",
    "                     \"files\" : [\n",
    "                         <relative_filename1>,\n",
    "                         <relative_filename2>,\n",
    "                         ...\n",
    "                     ],\n",
    "                     \"kwargs\": {\n",
    "                         <pandas_read_csv_kwarg1>: value_kwarg1,\n",
    "                         <pandas_read_csv_kwarg2>: value_kwarg2,\n",
    "                         ...\n",
    "                     } (if needed)                   \n",
    "                 },\n",
    "                 \"homogenize\": {\n",
    "                     \"column_mapping\": {\n",
    "                         <actual_column_name1_in_data_source>: <required_common_column_name1>, (if different)\n",
    "                         <actual_column_name2_in_data_source>: <required_common_column_name2>, (if different)\n",
    "                         ... \n",
    "                         None: [\n",
    "                             <required_common_column1_not_in_source>, \n",
    "                             <required_common_column2_not_in_source>,\n",
    "                             ...\n",
    "                         ] (if needed)\n",
    "                     },\n",
    "                     \"label_positive\": value (if needed)\n",
    "                     \"language\": value (if needed)\n",
    "                 }\n",
    "             },\n",
    "             ...\n",
    "            }\n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "    IntegrationSchemaType = Dict[str, Dict[str, Dict[str, Union[str, bool, List[str], Dict[str, str]]]]]\n",
    "    def integrate(self, schema: IntegrationSchemaType, save: bool = False ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Homogenize and integrate all the data sources specified in the integration schema\n",
    "\n",
    "        PARAMETERS:\n",
    "            - schema: Each key (data source folder name containing different files (train, test, all...)) \n",
    "              maps to both parameter sets (also dicts) required for read and homogenize properly. \n",
    "\n",
    "              The first dict (key \"read\") corresponds to the ones used by the function `pandas.read_csv` when reading \n",
    "              each file specified inside the folder. This dict must contain a list with each file name on the \n",
    "              key \"files\". It may also include another key (\"kwargs\") including other paramters needed when reading, \n",
    "              but only when necessary. The second dict (key \"homogenize\") corresponds to the keyword args used by \n",
    "              `SexismDataIntegrator.homogenize` and should contain \"column_mapping\" and \"label_positive\". Data source \n",
    "              folder name will be used for its `dataset_name_` parameter. For further understandig, please refer to \n",
    "              `SexismDataIntegrator.homogenize` documentation.\n",
    "\n",
    "              If one of the sources is multilingual, files are separated by language and there is not any column \n",
    "              containing such information, this source should appear as many times in the schema as languages are, \n",
    "              and each entry should by something like \"<data_source_folder_name>_<lang>\".\n",
    "\n",
    "              To better understand the concept, an schema of the schema can be found calling the defined method\n",
    "              `SexismDataIntegrator.integration_schema_template`.\n",
    "\n",
    "              - save: True for saving it to the self.trans_data_path\n",
    "\n",
    "        RETURNS: integrated homogenized data\n",
    "        \"\"\"\n",
    "        # buffer to store data sources homogenized\n",
    "        buffer = [] \n",
    "\n",
    "        # for each data source\n",
    "        for dataset_name_, kwargs in schema.items():\n",
    "\n",
    "            # prevent different naming when multilingual splitted cases\n",
    "            folder_name = dataset_name_.split(\"_\")[0]\n",
    "\n",
    "            # read data files contained in each data source\n",
    "            folder_path = f\"{self.orig_data_path}/{folder_name}/\"\n",
    "            kwargs_read = kwargs[\"read\"].get(\"kwargs\", {})\n",
    "            datasets = [ pd.read_csv(folder_path+file, **kwargs_read) for file in kwargs[\"read\"][\"files\"] ]\n",
    "\n",
    "            # homogenize data source\n",
    "            homogenized = self.homogenize(*datasets, **kwargs['homogenize'], dataset_name=folder_name)\n",
    "            buffer.append(homogenized)\n",
    "\n",
    "        # concat all homogenized data sources stored at the buffer\n",
    "        integrated = pd.concat(buffer, axis=0).reset_index(drop=True)\n",
    "        \n",
    "        # save reults\n",
    "        if save:\n",
    "            path = f\"{self.trans_data_path}/integrated_data.csv\"\n",
    "            integrated.to_csv(path, sep=\";\")\n",
    "            print(f\"Data correctly integrated and saved in \\033[1m'{path}'\\033[0m\")\n",
    "        \n",
    "        return integrated "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cff9a55-b2db-44d5-b436-a93af606ad21",
   "metadata": {},
   "source": [
    "**Custom Specification**\n",
    "\n",
    "The next step is to define the variables required for data integration according to our needs and create the `SexismDataIntegrator` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c7d659-6f22-4951-af1c-2a4b312e4237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contiene las carpetas de cada fuente de datos con sus respectivos ficheros\n",
    "ORIGINAL_DATA_PATH = \"../data/original\"\n",
    "\n",
    "# servirá para guardar los nuevos conjuntos de datos derivados de los anteriores\n",
    "TRANSFORMED_DATA_PATH = \"../data/transformed\"\n",
    "\n",
    "# definimos las columnas que queremos que tenga el conjunto de datos final\n",
    "# las claves son un identificador para esa columna y los valores el nombre que aparecerá\n",
    "# en el data frame. Debe contener la clave \"target\", donde se especifica la columna\n",
    "# que se necesita homogeneizar (pasar a numérico).\n",
    "COMMON_COLUMNS = collections.OrderedDict([(\"id\",\"original_id\"), \n",
    "                                          (\"txt\",\"text\"), \n",
    "                                          (\"target\",\"label\"), \n",
    "                                          (\"typ\", \"type\"),\n",
    "                                          (\"lang\", \"language\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d1cc5e-5d71-4c21-9ee8-b96bba45e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdi = SexismDataIntegrator(COMMON_COLUMNS, ORIGINAL_DATA_PATH, TRANSFORMED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb73d9-d157-475f-bf1a-40bbeb928b16",
   "metadata": {},
   "source": [
    "To define the integration schema, all we need to do is follow the template for each data source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63384eee-f91f-4d63-a2d1-4e06bac0a95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            schema = {\n",
      "             <data_source_folder_name_in_original_data_path>: {\n",
      "                 \"read\": {\n",
      "                     \"files\" : [\n",
      "                         <relative_filename1>,\n",
      "                         <relative_filename2>,\n",
      "                         ...\n",
      "                     ],\n",
      "                     \"kwargs\": {\n",
      "                         <pandas_read_csv_kwarg1>: value_kwarg1,\n",
      "                         <pandas_read_csv_kwarg2>: value_kwarg2,\n",
      "                         ...\n",
      "                     } (if needed)                   \n",
      "                 },\n",
      "                 \"homogenize\": {\n",
      "                     \"column_mapping\": {\n",
      "                         <actual_column_name1_in_data_source>: <required_common_column_name1>, (if different)\n",
      "                         <actual_column_name2_in_data_source>: <required_common_column_name2>, (if different)\n",
      "                         ... \n",
      "                         None: [\n",
      "                             <required_common_column1_not_in_source>, \n",
      "                             <required_common_column2_not_in_source>,\n",
      "                             ...\n",
      "                         ] (if needed)\n",
      "                     },\n",
      "                     \"label_positive\": value (if needed)\n",
      "                     \"language\": value (if needed)\n",
      "                 }\n",
      "             },\n",
      "             ...\n",
      "            }\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "sdi.integration_schema_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc08deb0-1a3f-4010-865b-91ca95f5642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTEGRATION_SCHEMA = {\n",
    "    \n",
    "    \"callme\": {\n",
    "        \"read\": {\n",
    "            \"files\": [\n",
    "                \"sexism_data.csv\"\n",
    "            ]\n",
    "        },\n",
    "        \"homogenize\": {\n",
    "            \"column_mapping\": {\n",
    "                \"id\": COMMON_COLUMNS[\"id\"],  \n",
    "                \"sexist\": COMMON_COLUMNS[\"target\"],\n",
    "                None: [\n",
    "                    COMMON_COLUMNS[\"typ\"]\n",
    "                ]\n",
    "            },\n",
    "            \"label_positive\": True,\n",
    "            \"language\": \"en\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"edos\": {\n",
    "        \"read\": {\n",
    "            \"files\": [\n",
    "                \"edos_labelled_aggregated.csv\"\n",
    "            ]\n",
    "        },\n",
    "        \"homogenize\": {\n",
    "            \"column_mapping\": {\n",
    "                \"rewire_id\": COMMON_COLUMNS[\"id\"],\n",
    "                \"label_sexist\": COMMON_COLUMNS[\"target\"],\n",
    "                \"label_category\": COMMON_COLUMNS[\"typ\"],\n",
    "            },\n",
    "            \"label_positive\": \"sexist\",\n",
    "            \"language\": \"en\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"evalita\": {\n",
    "        \"read\": {\n",
    "            \"files\": [\n",
    "                \"en_training.tsv\", \n",
    "                \"en_testing.tsv\"\n",
    "            ],\n",
    "            \"kwargs\": {\n",
    "                \"sep\": \"\\t\"\n",
    "            }\n",
    "        },\n",
    "        \"homogenize\": {\n",
    "            \"column_mapping\": {\n",
    "                \"id\": COMMON_COLUMNS[\"id\"], \n",
    "                \"misogynous\": COMMON_COLUMNS[\"target\"],\n",
    "                \"misogyny_category\": COMMON_COLUMNS[\"typ\"]\n",
    "            },\n",
    "            \"language\": \"en\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"exist\": {\n",
    "        \"read\": {\n",
    "            \"files\": [\n",
    "                \"EXIST2021_training.tsv\", \n",
    "                \"EXIST2021_test_labeled.tsv\"\n",
    "            ],\n",
    "            \"kwargs\": {\n",
    "                \"sep\": \"\\t\"\n",
    "            }\n",
    "        },\n",
    "        \"homogenize\": {\n",
    "            \"column_mapping\": {\n",
    "                \"id\": COMMON_COLUMNS[\"id\"], \n",
    "                \"task1\": COMMON_COLUMNS[\"target\"],\n",
    "                \"task2\": COMMON_COLUMNS[\"typ\"],\n",
    "                \"language\": COMMON_COLUMNS[\"lang\"]\n",
    "            },\n",
    "            \"label_positive\": \"sexist\",\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"ibereval_en\": {\n",
    "        \"read\": {\n",
    "            \"files\": [\n",
    "                \"en_AMI_TrainingSet_NEW.csv\"\n",
    "            ],\n",
    "            \"kwargs\": {\n",
    "                \"sep\": \";\"\n",
    "            }\n",
    "        },\n",
    "        \"homogenize\": {\n",
    "            \"column_mapping\": {\n",
    "                \"id\": COMMON_COLUMNS[\"id\"], \n",
    "                \"tweet\": COMMON_COLUMNS[\"txt\"], \n",
    "                \"misogynous\": COMMON_COLUMNS[\"target\"],\n",
    "                \"misogyny_category\": COMMON_COLUMNS[\"typ\"]\n",
    "            },\n",
    "            \"language\": \"en\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"ibereval_es\": {\n",
    "        \"read\": {\n",
    "            \"files\": [\n",
    "                \"es_AMI_TrainingSet_NEW.csv\"\n",
    "            ],\n",
    "            \"kwargs\": {\n",
    "                \"sep\": \";\"\n",
    "            }\n",
    "        },\n",
    "        \"homogenize\": {\n",
    "            \"column_mapping\": {\n",
    "                \"id\": COMMON_COLUMNS[\"id\"], \n",
    "                \"tweet\": COMMON_COLUMNS[\"txt\"], \n",
    "                \"misogynous\": COMMON_COLUMNS[\"target\"],\n",
    "                \"misogyny_category\": COMMON_COLUMNS[\"typ\"]\n",
    "            },\n",
    "            \"language\": \"es\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"metwo\": {\n",
    "        \"read\": {\n",
    "            \"files\": [\n",
    "                \"targetResultFile_full2.csv\"\n",
    "            ],\n",
    "            \"kwargs\": {\n",
    "                \"sep\": \";\", \n",
    "                \"names\": [\n",
    "                    COMMON_COLUMNS[\"id\"], \n",
    "                    COMMON_COLUMNS[\"txt\"], \n",
    "                    COMMON_COLUMNS[\"target\"]\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"homogenize\": {\n",
    "            \"column_mapping\": {\n",
    "                None: [\n",
    "                    COMMON_COLUMNS[\"typ\"]\n",
    "                ]\n",
    "            },\n",
    "            \"label_positive\": \"SEXIST\",\n",
    "            \"language\": \"es\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eea564c-5aff-4ce9-b4d5-543a1e994f93",
   "metadata": {},
   "source": [
    "With the integration schema properly defined, all that's left is to call the `integrate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d00b709-5187-42df-b993-736bc9eece98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data correctly integrated and saved in \u001b[1m'../data/transformed/integrated_data.csv'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MENTION3481 i didn't even know random was an o...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Bottom two should've gone!  #mkr</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MENTION3111 MENTION3424 ladyboner deserves so ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>She shall now be known as Sourpuss #MKR #KatAn...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Tarah W threw a bunch of women under the bus s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59000</th>\n",
       "      <td>1047687262455177217</td>\n",
       "      <td>Yo no puedo darte luz todos los días, pero si ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59001</th>\n",
       "      <td>1064482731739045888</td>\n",
       "      <td>Que bien! Aunque digan que las mujeres no debe...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59002</th>\n",
       "      <td>1040584804536856577</td>\n",
       "      <td>@AriOrsingher Y misoginia las pelotas no quier...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59003</th>\n",
       "      <td>1051458429280235520</td>\n",
       "      <td>\"Imaginen el tipo de sociedad mojigata y castr...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59004</th>\n",
       "      <td>1020717495605489673</td>\n",
       "      <td>@SenadoraBlas por favor pase a la historia com...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59005 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               original_id                                               text  \\\n",
       "0                        0  MENTION3481 i didn't even know random was an o...   \n",
       "1                        1                   Bottom two should've gone!  #mkr   \n",
       "2                        2  MENTION3111 MENTION3424 ladyboner deserves so ...   \n",
       "3                        3  She shall now be known as Sourpuss #MKR #KatAn...   \n",
       "4                        4  Tarah W threw a bunch of women under the bus s...   \n",
       "...                    ...                                                ...   \n",
       "59000  1047687262455177217  Yo no puedo darte luz todos los días, pero si ...   \n",
       "59001  1064482731739045888  Que bien! Aunque digan que las mujeres no debe...   \n",
       "59002  1040584804536856577  @AriOrsingher Y misoginia las pelotas no quier...   \n",
       "59003  1051458429280235520  \"Imaginen el tipo de sociedad mojigata y castr...   \n",
       "59004  1020717495605489673  @SenadoraBlas por favor pase a la historia com...   \n",
       "\n",
       "       label  type language dataset  \n",
       "0          0  None       en  callme  \n",
       "1          0  None       en  callme  \n",
       "2          0  None       en  callme  \n",
       "3          0  None       en  callme  \n",
       "4          0  None       en  callme  \n",
       "...      ...   ...      ...     ...  \n",
       "59000      0  None       es   metwo  \n",
       "59001      0  None       es   metwo  \n",
       "59002      0  None       es   metwo  \n",
       "59003      0  None       es   metwo  \n",
       "59004      0  None       es   metwo  \n",
       "\n",
       "[59005 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integrated_data = sdi.integrate(INTEGRATION_SCHEMA, save=True)\n",
    "integrated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc97560-e3f7-44ce-a0bf-a47afcd3456d",
   "metadata": {},
   "source": [
    "And some relevant proportions and counts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45a1f2af-9ee8-4fd4-a9d1-acf5d65426b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-sexist: 68.53%\n",
      "Sexist: 31.47%\n"
     ]
    }
   ],
   "source": [
    "count = integrated_data.label.sum()\n",
    "total = len(integrated_data)\n",
    "print(f\"Non-sexist: {round((total-count)/total*100, 2)}%\\nSexist: {round(count/total*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f223ce8-bb3a-4449-b416-1589256568b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: 80.55%\n",
      "Spanish: 19.45%\n"
     ]
    }
   ],
   "source": [
    "count = (integrated_data.language == \"en\").sum()\n",
    "print(f\"English: {round(count/total*100, 2)}%\\nSpanish: {round((total-count)/total*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "965514d7-3372-4731-9e69-e00da7a999c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>language</th>\n",
       "      <th>English</th>\n",
       "      <th>Spanish</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Non-Sexist</th>\n",
       "      <td>34256</td>\n",
       "      <td>6182</td>\n",
       "      <td>40438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sexist</th>\n",
       "      <td>13270</td>\n",
       "      <td>5297</td>\n",
       "      <td>18567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>47526</td>\n",
       "      <td>11479</td>\n",
       "      <td>59005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "language    English  Spanish    All\n",
       "label                              \n",
       "Non-Sexist    34256     6182  40438\n",
       "Sexist        13270     5297  18567\n",
       "All           47526    11479  59005"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(\n",
    "    integrated_data.label.apply(int2label, positive=\"Sexist\", negative=\"Non-Sexist\"),\n",
    "    integrated_data.language.apply(lambda x: \"English\" if x == \"en\" else \"Spanish\"),\n",
    "    margins = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4646e587-dae4-4908-bf66-100ff2793c5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f339e53-eb72-44c7-a0bd-ee17583ad12f",
   "metadata": {},
   "source": [
    "Next step in our pre-process is cleaning text properly. For this task function `clean` is defined, which handles case, blanks, numbers, links, mentions, hashtags, emojis, accents, symbols and punctuation. It allows customizing links, mentions, hashtag and emojis handling and adding custom regexs and its substitution in cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf28c248-e250-4c1c-88ab-99dc16f62f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(\n",
    "    \n",
    "        text: str, \n",
    "        keep_case: bool = False,\n",
    "        keep_accents: bool = False,\n",
    "        keep_numbers: bool = False,\n",
    "        lmhe_tokens: Optional[Dict[str, str]] = None, \n",
    "        constraints: Optional[List[Tuple[str, str]]] = None,\n",
    "        allowed_punctuation: Optional[str] = None\n",
    "    \n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Clean a given text\n",
    "    \n",
    "    PARAMETERS:\n",
    "        - text: string to clean\n",
    "        - keep_case: wether to keep original case (True) or not (False). If not text will be lowercased.\n",
    "        - keep_accents: wether to keep accents (True) or not (False).\n",
    "        - keep_numbers: wether to keep numbers (True) or not (False).\n",
    "        - lmhe_tokens: which stands for link-mention-hashtag-emoji_tokens. A dict containing how to represent.\n",
    "          those items in the final text. If nothing provided (neither dict or specific key), they will be removed.\n",
    "        - constraints: any special substitution you may want to apply to the text. It must be a list of tuples \n",
    "          containing the corresponding regex to capture (first element of the tuple) and the string to substitue \n",
    "          it (second element).\n",
    "        - allowed_punctuation: string containing custom punctuation you may want to avoid cleaning.\n",
    "        \n",
    "    RETURNS cleaned text\n",
    "    \"\"\"\n",
    "    \n",
    "    # lowercase\n",
    "    if not keep_case:\n",
    "        text = text.lower() \n",
    "    \n",
    "    #remove \\n and \\r\n",
    "    text = text.replace('\\r', '').replace('\\n', ' ')\n",
    "    \n",
    "    if lmhe_tokens is not None:\n",
    "        # handle links\n",
    "        text = re.sub(r'(?:www\\.|https?://)\\S+', lmhe_tokens.get(\"link\", ''), text, flags=re.MULTILINE)  \n",
    "        \n",
    "        # handle mentions\n",
    "        text = re.sub(r'\\@\\S+', lmhe_tokens.get(\"mention\", ''), text) \n",
    "        \n",
    "        # handle hashtags\n",
    "        text = re.sub(r'#\\S+', lmhe_tokens.get(\"hashtag\", ''), text)\n",
    "        \n",
    "        # handle emojis\n",
    "        text = emoji.replace_emoji(text, lmhe_tokens.get(\"emoji\", ''))  \n",
    "        \n",
    "    else:\n",
    "        # remove links, mentions, hashtags and emojis\n",
    "        text = re.sub(r'(?:#|\\@|www\\.|https?://)\\S+', '', text, flags=re.MULTILINE) \n",
    "        text = emoji.replace_emoji(text, '')\n",
    "     \n",
    "    # specific constraints\n",
    "    if constraints is not None:\n",
    "        for regex, token in constraints:\n",
    "            text = re.sub(regex, token, text, flags=re.I)\n",
    "    \n",
    "    # remove accents\n",
    "    if not keep_accents:\n",
    "        text = unidecode.unidecode(text)  \n",
    "    \n",
    "    ## all symbols and punctuation\n",
    "    banned_list = string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'+'—'  \n",
    "    ## allowed punctuation\n",
    "    if allowed_punctuation is not None:  \n",
    "        banned_list = re.sub(r\"[%s]\" % re.escape(allowed_punctuation), \"\", banned_list)\n",
    "    # remove symbols and punctuation\n",
    "    text = text.translate(str.maketrans('', '', banned_list)) \n",
    "    \n",
    "    # remove numbers\n",
    "    if not keep_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)  \n",
    "    \n",
    "    # remove extra and leading blanks\n",
    "    text = re.sub(\"\\s\\s+\" , \" \", text).strip()  \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca584bbd-a806-40cd-a115-43fb7a260db2",
   "metadata": {},
   "source": [
    "We want to maintain the following punctuation because the models we are going to use had been trained with almost raw text and well know how to interpret them. Also, exclamation and interrogation marks could express some emotions. We maintain inverted commas (') because we don't want cases such \"should've\" -> \"shouldve\" on English data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f9ea78d-201e-405b-aa29-93d25f97d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_PUNCTUATION = \"'\\\"!¿?.,\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e97f40-87d5-4ac9-8df3-ac194b4a6557",
   "metadata": {},
   "source": [
    "We are going to maintain accents because of the spanish data and because BETO also handles them. Our constraints came from EDOS texts (links as \"\\[URL\\]\" and users as \"\\[USER\\]\") and CALLME texts (mentions as \"MENTION\\<number\\>\" and re-tweets as \"RT\"), since we want to discard those information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a05ff13f-0fb5-44d8-ba1c-205c3eb7a4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i didn't even know random was an option!</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bottom two should've gone!</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ladyboner deserves so much more credit than du...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>she shall now be known as sourpuss</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tarah w threw a bunch of women under the bus s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59000</th>\n",
       "      <td>1047687262455177217</td>\n",
       "      <td>yo no puedo darte luz todos los días, pero si ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59001</th>\n",
       "      <td>1064482731739045888</td>\n",
       "      <td>que bien! aunque digan que las mujeres no debe...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59002</th>\n",
       "      <td>1040584804536856577</td>\n",
       "      <td>y misoginia las pelotas no quiero que vengas a...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59003</th>\n",
       "      <td>1051458429280235520</td>\n",
       "      <td>\"imaginen el tipo de sociedad mojigata y castr...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59004</th>\n",
       "      <td>1020717495605489673</td>\n",
       "      <td>por favor pase a la historia como una mujer qu...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58762 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               original_id                                               text  \\\n",
       "0                        0           i didn't even know random was an option!   \n",
       "1                        1                         bottom two should've gone!   \n",
       "2                        2  ladyboner deserves so much more credit than du...   \n",
       "3                        3                 she shall now be known as sourpuss   \n",
       "4                        4  tarah w threw a bunch of women under the bus s...   \n",
       "...                    ...                                                ...   \n",
       "59000  1047687262455177217  yo no puedo darte luz todos los días, pero si ...   \n",
       "59001  1064482731739045888  que bien! aunque digan que las mujeres no debe...   \n",
       "59002  1040584804536856577  y misoginia las pelotas no quiero que vengas a...   \n",
       "59003  1051458429280235520  \"imaginen el tipo de sociedad mojigata y castr...   \n",
       "59004  1020717495605489673  por favor pase a la historia como una mujer qu...   \n",
       "\n",
       "       label  type language dataset  \n",
       "0          0  None       en  callme  \n",
       "1          0  None       en  callme  \n",
       "2          0  None       en  callme  \n",
       "3          0  None       en  callme  \n",
       "4          0  None       en  callme  \n",
       "...      ...   ...      ...     ...  \n",
       "59000      0  None       es   metwo  \n",
       "59001      0  None       es   metwo  \n",
       "59002      0  None       es   metwo  \n",
       "59003      0  None       es   metwo  \n",
       "59004      0  None       es   metwo  \n",
       "\n",
       "[58762 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONSTRAINTS = [(r\"\\[URL\\]|\\[USER\\]\", \"\"), (r\"MENTION\\d+\", \"\"), (r\"\\bRT\\b\", \"\")]\n",
    "cleaned_data = integrated_data.copy()\n",
    "cleaned_data.text = cleaned_data.text.apply(clean, \n",
    "                                            keep_case=False,\n",
    "                                            keep_accents=True,\n",
    "                                            keep_numbers=True,\n",
    "                                            constraints=CONSTRAINTS, \n",
    "                                            allowed_punctuation=ALLOWED_PUNCTUATION)\n",
    "\n",
    "#because of the transformations it is posible that some texts get completly empty\n",
    "cleaned_data = cleaned_data.loc[cleaned_data.text != \"\", :]\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7adb25d-5d62-4372-9fe5-bf67e71bcd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.to_csv(f\"{TRANSFORMED_DATA_PATH}/cleaned_data.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea8b3c1-837a-4963-a27b-9178bd25f79a",
   "metadata": {},
   "source": [
    "Another posibility could be to maintain links, mentions, hashtags and emojis as special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5712f2f0-136a-4e20-b928-112a7ddc30a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[USER] i didn't even know random was an option!</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bottom two should've gone! [HASHTAG]</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[USER] [USER] ladyboner deserves so much more ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>she shall now be known as sourpuss [HASHTAG] [...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tarah w threw a bunch of women under the bus s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59000</th>\n",
       "      <td>1047687262455177217</td>\n",
       "      <td>yo no puedo darte luz todos los días, pero si ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59001</th>\n",
       "      <td>1064482731739045888</td>\n",
       "      <td>que bien! aunque digan que las mujeres no debe...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59002</th>\n",
       "      <td>1040584804536856577</td>\n",
       "      <td>[USER] y misoginia las pelotas no quiero que v...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59003</th>\n",
       "      <td>1051458429280235520</td>\n",
       "      <td>\"imaginen el tipo de sociedad mojigata y castr...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59004</th>\n",
       "      <td>1020717495605489673</td>\n",
       "      <td>[USER] por favor pase a la historia como una m...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59005 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               original_id                                               text  \\\n",
       "0                        0    [USER] i didn't even know random was an option!   \n",
       "1                        1               bottom two should've gone! [HASHTAG]   \n",
       "2                        2  [USER] [USER] ladyboner deserves so much more ...   \n",
       "3                        3  she shall now be known as sourpuss [HASHTAG] [...   \n",
       "4                        4  tarah w threw a bunch of women under the bus s...   \n",
       "...                    ...                                                ...   \n",
       "59000  1047687262455177217  yo no puedo darte luz todos los días, pero si ...   \n",
       "59001  1064482731739045888  que bien! aunque digan que las mujeres no debe...   \n",
       "59002  1040584804536856577  [USER] y misoginia las pelotas no quiero que v...   \n",
       "59003  1051458429280235520  \"imaginen el tipo de sociedad mojigata y castr...   \n",
       "59004  1020717495605489673  [USER] por favor pase a la historia como una m...   \n",
       "\n",
       "       label  type language dataset  \n",
       "0          0  None       en  callme  \n",
       "1          0  None       en  callme  \n",
       "2          0  None       en  callme  \n",
       "3          0  None       en  callme  \n",
       "4          0  None       en  callme  \n",
       "...      ...   ...      ...     ...  \n",
       "59000      0  None       es   metwo  \n",
       "59001      0  None       es   metwo  \n",
       "59002      0  None       es   metwo  \n",
       "59003      0  None       es   metwo  \n",
       "59004      0  None       es   metwo  \n",
       "\n",
       "[59005 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONSTRAINTS = [(r\"MENTION\\d+\", \"[USER]\"), (r\"\\bRT\\b\", \"\")]\n",
    "LMHE_TOKENS = {\n",
    "    \"link\": \"[URL]\", \n",
    "    \"mention\": \"[USER]\", \n",
    "    \"hashtag\": \"[HASHTAG]\", \n",
    "    \"emoji\": \"[EMOJI]\"\n",
    "}\n",
    "\n",
    "cleaned_data2 = integrated_data.copy()\n",
    "cleaned_data2.text = cleaned_data2.text.apply(clean, \n",
    "                                              keep_case=False,\n",
    "                                              keep_accents=True,\n",
    "                                              keep_numbers=True,\n",
    "                                              constraints=CONSTRAINTS,  \n",
    "                                              lmhe_tokens=LMHE_TOKENS, \n",
    "                                              allowed_punctuation=ALLOWED_PUNCTUATION+\"[]\")\n",
    "cleaned_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c0efb12-78ac-4c25-bd17-a7fda1fe367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data2.to_csv(f\"{TRANSFORMED_DATA_PATH}/cleaned_data_with_lmhe_tokens.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb753c9-9251-4b16-95db-6a146ab54e6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e796e1-0803-4931-b613-fb579e88380b",
   "metadata": {},
   "source": [
    "First step is to select English data to be translated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23c8f2af-2f7f-4ae6-85c6-395caf7838b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.read_csv(f\"{TRANSFORMED_DATA_PATH}/cleaned_data.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "415c2c19-47ab-4197-a1b5-f6313bca2b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i didn't even know random was an option!</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bottom two should've gone!</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ladyboner deserves so much more credit than du...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>she shall now be known as sourpuss</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tarah w threw a bunch of women under the bus s...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52996</th>\n",
       "      <td>3230</td>\n",
       "      <td>when someone announces they're unfollowing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>ibereval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52997</th>\n",
       "      <td>3265</td>\n",
       "      <td>when someone announces that they are 'official...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>ibereval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52998</th>\n",
       "      <td>3387</td>\n",
       "      <td>deleted again. working to get it back again</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>ibereval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52999</th>\n",
       "      <td>3259</td>\n",
       "      <td>when you're on a first date and she asks to ta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>ibereval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53000</th>\n",
       "      <td>3525</td>\n",
       "      <td>when your friend won't stop talking about thei...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>ibereval</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47319 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      original_id                                               text  label  \\\n",
       "0               0           i didn't even know random was an option!      0   \n",
       "1               1                         bottom two should've gone!      0   \n",
       "2               2  ladyboner deserves so much more credit than du...      0   \n",
       "3               3                 she shall now be known as sourpuss      0   \n",
       "4               4  tarah w threw a bunch of women under the bus s...      0   \n",
       "...           ...                                                ...    ...   \n",
       "52996        3230         when someone announces they're unfollowing      0   \n",
       "52997        3265  when someone announces that they are 'official...      0   \n",
       "52998        3387        deleted again. working to get it back again      0   \n",
       "52999        3259  when you're on a first date and she asks to ta...      0   \n",
       "53000        3525  when your friend won't stop talking about thei...      0   \n",
       "\n",
       "      type language   dataset  \n",
       "0      NaN       en    callme  \n",
       "1      NaN       en    callme  \n",
       "2      NaN       en    callme  \n",
       "3      NaN       en    callme  \n",
       "4      NaN       en    callme  \n",
       "...    ...      ...       ...  \n",
       "52996    0       en  ibereval  \n",
       "52997    0       en  ibereval  \n",
       "52998    0       en  ibereval  \n",
       "52999    0       en  ibereval  \n",
       "53000    0       en  ibereval  \n",
       "\n",
       "[47319 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data_en = cleaned_data.loc[cleaned_data.language == \"en\"]\n",
    "cleaned_data_en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c35ea4-d80f-4708-a8dc-294207847784",
   "metadata": {},
   "source": [
    "Let's first look at how many tokens are we going to need and the costs associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c0e9fd2-3c7a-4172-8c5f-32a27049fb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 1163576\n",
      "Average: 24.590037828356476\n",
      "Median: 22.0\n",
      "Mode: 16\n",
      "Max: 760\n",
      "Min: 1\n",
      "Std: 16.285526457021238\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import scipy.stats as st \n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "tokens = np.array([ len(encoding.encode(row.text)) for i, row in cleaned_data_en.iterrows() ])\n",
    "print(f\"Total: {tokens.sum()}\\n\"\n",
    "      f\"Average: {tokens.mean()}\\n\"\n",
    "      f\"Median: {np.median(tokens)}\\n\"\n",
    "      f\"Mode: {st.mode(tokens).mode[0]}\\n\"\n",
    "      f\"Max: {tokens.max()}\\n\"\n",
    "      f\"Min: {tokens.min()}\\n\"\n",
    "      f\"Std: {tokens.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea0c20c8-d528-42bc-bbe1-c171b63ec7e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price for translating 1163576 tokens:\n",
      " • GPT-3.5 turbo ($0.002/1K): $4.65 aprox.\n",
      " • GPT-4 ($0.03/1K): $69.81 aprox.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Price for translating {tokens.sum()} tokens:\\n\"\n",
    "      f\" {chr(8226)} GPT-3.5 turbo ($0.002/1K): ${round(tokens.sum()*0.002/1000*2, 2)} aprox.\\n\"\n",
    "      f\" {chr(8226)} GPT-4 ($0.03/1K): ${round(tokens.sum()*0.03/1000*2, 2)} aprox.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba294e08-e88e-40ed-b25d-0ac10a56bb1e",
   "metadata": {},
   "source": [
    "### Using GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e669661-6f70-4f7e-8c81-b60ca1edcba6",
   "metadata": {},
   "source": [
    "It is important to define some constant variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26081a7a-6189-438d-9033-e16cd4206064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"OPENAI_API_KEY\"] = \"<your openai apikey>\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "MAX_TOKENS = 4096\n",
    "RPM = 3  # requests per minute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a854166-0100-4490-bb98-4c3cd90e4369",
   "metadata": {},
   "source": [
    "And to define how our translation process is going to be held. It is important to take into account the extremly large amount of teets we have to translate. Another thing to consider its that making single requests per tweet to the GPT model it worth less, considering that a lot of token can be passed to the API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d43e7a7-a084-4d3e-9ec1-b29404d74b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prettytime(t:int):\n",
    "    \"\"\"Dada una cantidad t de segundos, lo devuelve en formato hh:mm:ss\"\"\"\n",
    "    hh, mm = divmod(t, 3600)\n",
    "    mm, ss = divmod(mm, 60)\n",
    "    hh, mm, ss = map(lambda x: str(int(x)), [hh,mm,ss])\n",
    "    return f'{hh.zfill(2)}:{mm.zfill(2)}:{ss.zfill(2)}'\n",
    "\n",
    "def split_text_given_max_tokens(texts: pd.Series, max_tokens: int) -> Iterator[Tuple[list, list]]:\n",
    "    \"\"\"\n",
    "    Split long pd.Series of texts into small batches optimized to fit into max_tokens allowed.\n",
    "    \n",
    "    PARAMETERS:\n",
    "        - texts: (large amount of) indexed tweets to be splitted.\n",
    "        - max_tokens: maximum number of GPT tokens (computed using tiktoken.encoding_for_model.encode method).\n",
    "        \n",
    "    YIELDS batches of text with its respective indexes (ids)\n",
    "    \"\"\"\n",
    "    \n",
    "    ids, batch, aux = [], [], 0\n",
    "    encoding = tiktoken.encoding_for_model(GPT_MODEL)\n",
    "    for i, txt in texts.items():\n",
    "        len_tweet = len(encoding.encode(txt))\n",
    "        if aux +  len_tweet < max_tokens:\n",
    "            batch.append(txt)\n",
    "            ids.append(i)\n",
    "            aux += len_tweet\n",
    "        else:\n",
    "            yield ids, batch\n",
    "            ids, batch, aux = [], [], 0\n",
    "    if batch:\n",
    "        yield ids, batch\n",
    "\n",
    "        \n",
    "def gpt_translation(texts: list, input_language: str, output_language: str, model: str) -> list:\n",
    "    \"\"\"\n",
    "    Translate text using OpenAI's GPT. \n",
    "    \n",
    "    PARAMETERS:\n",
    "        - texts: list of raw text to be translated (total tokens contained must not exceed 1500 aprox)\n",
    "        - input_language: original text language.\n",
    "        - output_language: language to be translated.\n",
    "        - model: GPT model name (expected for openai.ChatCompletion.create method)\n",
    "        \n",
    "    RETURNS translated texts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set the system message for GPT model\n",
    "    context = (\n",
    "        f\"Assistant is an intelligent chatbot designed to translate {input_language} tweets into {output_language}.\\n\" \\\n",
    "        \"Instructions:\\n\" \\\n",
    "        \" - You will be provided with one tweet per line. Each line contains tweet's id (first number) and the tweet.\\n\" \\\n",
    "        \" - Ensure that the translations are accurate and preserve the original meaning and tone of each tweet.\\n\" \\\n",
    "        \" - Take into account any slang or informal language used in the tweets, as well as any potential variations in spelling or grammar.\" \\\n",
    "        \" - Each line in your response must correspond to each tweet provided and in the same order.\\n\" \\\n",
    "        \" - If you are not able to translate one of them, return just its id in that position.\"\n",
    "    )\n",
    "    \n",
    "    # ask GPT for the translation\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": context},\n",
    "            {\"role\": \"user\", \"content\": \"\\n\".join(texts)}\n",
    "        ]\n",
    "    ).choices[0].message.content.split(\"\\n\")\n",
    "    \n",
    "\n",
    "TRANSLATION_BUFFER, IDS_BUFFER, ORIGINAL_IDS_BUFFER = [], [], []\n",
    "def translate(texts: pd.Series, original: str, new: str, verbose: bool = False, outappend: bool = False) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Translate the pandas Series passed.\n",
    "    \n",
    "    PARAMETERS:\n",
    "        - texts: indexed tweets to be translated\n",
    "        - original: data source language\n",
    "        - new: language get translations\n",
    "        \n",
    "    RETURNS translated Series (maintaining tweets original associated indexes)\n",
    "    \"\"\"\n",
    "    if outappend:\n",
    "        global TRANS_BUFFER\n",
    "    translated, translated_ids, original_ids = [], [], []\n",
    "    time_buffer, requests_buffer = 0, 0\n",
    "    \n",
    "    # it is needed to left some tokens for other parts of the query just than texts to translate (substract 1000 to MAX_TOKENS).\n",
    "    # morover, considering that the max token allowed for the model include both input and output token, we take the half.\n",
    "    allowed_tokens = int((MAX_TOKENS-1000)/2)\n",
    "    \n",
    "    # for each batch od tweets\n",
    "    for i, (ids, batch) in enumerate(split_text_given_max_tokens(texts, allowed_tokens)):\n",
    "        t0 = time.time()\n",
    "\n",
    "        # transform texts into the form <id_tweet><blank_space><tweet> for a better correspondance with response\n",
    "        proper_input = pd.Series(ids).astype(str) + \" \" + pd.Series(batch)\n",
    "        # ask gpt to translate the texts\n",
    "        response = gpt_translation(proper_input, input_language=original, output_language=new, model=GPT_MODEL)\n",
    "        \n",
    "        # for each translation (response is a list of strings)\n",
    "        retrieved, correct, incorrect = 0, 0, 0\n",
    "        for sent in response:\n",
    "            \n",
    "            # normal cases in the same format as the input\n",
    "            try:\n",
    "                pos = sent.index(\" \")  ## matches with the first blank space (to split the sentence into id and tweet)\n",
    "                id_ = sent[:pos].strip(\".\").strip(\",\") ## get tweet id\n",
    "                trans = sent[pos+1:]  ## get translated tweet\n",
    "                correct += 1\n",
    "            \n",
    "            # no spaces, just a number, gpt model did not return sentence translation\n",
    "            except ValueError: \n",
    "                id_ = sent.strip(\".\").strip(\",\")  ## get id tweet\n",
    "                trans = \"\"  ## no text translated\n",
    "                incorrect +=1\n",
    "            \n",
    "            # accumulate ids and text from response\n",
    "            translated_ids.append(id_)\n",
    "            IDS_BUFFER.append(id_)\n",
    "            translated.append(trans)\n",
    "            TRANSLATION_BUFFER.append(trans)\n",
    "            retrieved += 1\n",
    "        \n",
    "        # accumulate original ids for checking purposes\n",
    "        original_ids.extend(ids) \n",
    "        ORIGINAL_IDS_BUFFER.extend(ids)\n",
    "\n",
    "        t1 = time.time()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Batch: {i:3} \" \\\n",
    "                  f\"| Sent: {len(ids)} \" \\\n",
    "                  f\"| Retrieved: {retrieved}; Translated: {round(correct/retrieved,2)*100}% \" \\\n",
    "                  f\"| Time: {prettytime(t1-t0)}\")\n",
    "        \n",
    "        # to control that there aren't more than the permitted requests to the API\n",
    "        time_buffer += t1-t0\n",
    "        requests_buffer += 1\n",
    "        if requests_buffer == RPM:\n",
    "            if time_buffer < 60:\n",
    "                time.sleep(60-time_buffer)\n",
    "            time_buffer, requests_buffer = 0, 0\n",
    "            \n",
    "    #return pd.Series(translated, index=translated_ids), original_ids        \n",
    "    return pd.DataFrame({\"original_id\":translated_ids, \"translated\": translated}), original_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714d0f8-4900-44fd-9c10-6e1e1eb0355d",
   "metadata": {},
   "source": [
    "Once we have the functions defined we only have to select our tweets and pass trought `translate`.\n",
    "\n",
    "> **Note**: At this moment this section of the notebooks is being executed in other machine due to its high amount of time required. It is going to take about 750 batches (between 22 and 55 hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c06dfe-b83b-4a0b-a1a5-d5883932801f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 | Sended: 93 | Retrieved: 93; Translated: 100.0% | Time: 00:03:19\n",
      "Batch: 1 | Sended: 86 | Retrieved: 86; Translated: 100.0% | Time: 00:03:06\n",
      "Batch: 2 | Sended: 87 | Retrieved: 86; Translated: 100.0% | Time: 00:03:13\n",
      "Batch: 3 | Sended: 93 | Retrieved: 93; Translated: 100.0% | Time: 00:03:18\n",
      "Batch: 4 | Sended: 86 | Retrieved: 85; Translated: 100.0% | Time: 00:03:16\n",
      "Batch: 5 | Sended: 95 | Retrieved: 92; Translated: 100.0% | Time: 00:03:10\n",
      "Batch: 6 | Sended: 92 | Retrieved: 92; Translated: 100.0% | Time: 00:03:06\n",
      "Batch: 7 | Sended: 86 | Retrieved: 76; Translated: 100.0% | Time: 00:02:48\n",
      "Batch: 8 | Sended: 91 | Retrieved: 90; Translated: 100.0% | Time: 00:03:07\n",
      "Batch: 9 | Sended: 86 | Retrieved: 82; Translated: 100.0% | Time: 00:02:10\n",
      "Batch: 10 | Sended: 88 | Retrieved: 88; Translated: 100.0% | Time: 00:03:17\n",
      "Batch: 11 | Sended: 87 | Retrieved: 86; Translated: 100.0% | Time: 00:03:21\n",
      "Batch: 12 | Sended: 91 | Retrieved: 89; Translated: 100.0% | Time: 00:03:18\n",
      "Batch: 13 | Sended: 88 | Retrieved: 86; Translated: 100.0% | Time: 00:03:08\n",
      "Batch: 14 | Sended: 89 | Retrieved: 86; Translated: 100.0% | Time: 00:03:16\n",
      "Batch: 15 | Sended: 89 | Retrieved: 87; Translated: 100.0% | Time: 00:03:01\n",
      "Batch: 16 | Sended: 89 | Retrieved: 86; Translated: 100.0% | Time: 00:03:05\n",
      "Batch: 17 | Sended: 92 | Retrieved: 88; Translated: 100.0% | Time: 00:03:11\n",
      "Batch: 18 | Sended: 95 | Retrieved: 92; Translated: 100.0% | Time: 00:03:14\n",
      "Batch: 19 | Sended: 90 | Retrieved: 87; Translated: 100.0% | Time: 00:03:08\n",
      "Batch: 20 | Sended: 97 | Retrieved: 93; Translated: 100.0% | Time: 00:03:05\n",
      "Batch: 21 | Sended: 90 | Retrieved: 90; Translated: 100.0% | Time: 00:02:20\n",
      "Batch: 22 | Sended: 95 | Retrieved: 94; Translated: 100.0% | Time: 00:03:11\n",
      "Batch: 23 | Sended: 92 | Retrieved: 86; Translated: 100.0% | Time: 00:03:08\n",
      "Batch: 24 | Sended: 86 | Retrieved: 85; Translated: 100.0% | Time: 00:03:08\n",
      "Batch: 25 | Sended: 90 | Retrieved: 88; Translated: 100.0% | Time: 00:03:08\n",
      "Batch: 26 | Sended: 98 | Retrieved: 89; Translated: 100.0% | Time: 00:03:25\n",
      "Batch: 27 | Sended: 92 | Retrieved: 91; Translated: 100.0% | Time: 00:03:00\n",
      "Batch: 28 | Sended: 88 | Retrieved: 88; Translated: 100.0% | Time: 00:03:04\n",
      "Batch: 29 | Sended: 91 | Retrieved: 90; Translated: 100.0% | Time: 00:03:07\n",
      "Batch: 30 | Sended: 90 | Retrieved: 90; Translated: 100.0% | Time: 00:03:04\n",
      "Batch: 31 | Sended: 93 | Retrieved: 91; Translated: 100.0% | Time: 00:03:05\n",
      "Batch: 32 | Sended: 92 | Retrieved: 89; Translated: 100.0% | Time: 00:03:05\n",
      "Batch: 33 | Sended: 88 | Retrieved: 86; Translated: 100.0% | Time: 00:03:11\n",
      "Batch: 34 | Sended: 96 | Retrieved: 92; Translated: 100.0% | Time: 00:03:01\n",
      "Batch: 35 | Sended: 90 | Retrieved: 87; Translated: 100.0% | Time: 00:03:00\n",
      "Batch: 36 | Sended: 90 | Retrieved: 90; Translated: 100.0% | Time: 00:03:01\n",
      "Batch: 37 | Sended: 93 | Retrieved: 92; Translated: 99.0% | Time: 00:03:03\n",
      "Batch: 38 | Sended: 102 | Retrieved: 97; Translated: 100.0% | Time: 00:02:53\n",
      "Batch: 39 | Sended: 93 | Retrieved: 90; Translated: 100.0% | Time: 00:02:41\n",
      "Batch: 40 | Sended: 87 | Retrieved: 87; Translated: 100.0% | Time: 00:02:49\n",
      "Batch: 41 | Sended: 94 | Retrieved: 90; Translated: 100.0% | Time: 00:02:42\n",
      "Batch: 42 | Sended: 86 | Retrieved: 86; Translated: 100.0% | Time: 00:02:43\n",
      "Batch: 43 | Sended: 101 | Retrieved: 94; Translated: 100.0% | Time: 00:02:36\n",
      "Batch: 44 | Sended: 97 | Retrieved: 92; Translated: 100.0% | Time: 00:02:40\n",
      "Batch: 45 | Sended: 92 | Retrieved: 91; Translated: 100.0% | Time: 00:02:38\n",
      "Batch: 46 | Sended: 99 | Retrieved: 97; Translated: 99.0% | Time: 00:02:34\n",
      "Batch: 47 | Sended: 95 | Retrieved: 56; Translated: 98.0% | Time: 00:01:57\n",
      "Batch: 48 | Sended: 95 | Retrieved: 70; Translated: 100.0% | Time: 00:02:00\n",
      "Batch: 49 | Sended: 94 | Retrieved: 86; Translated: 100.0% | Time: 00:02:42\n",
      "Batch: 50 | Sended: 98 | Retrieved: 94; Translated: 100.0% | Time: 00:02:41\n",
      "Batch: 51 | Sended: 87 | Retrieved: 84; Translated: 99.0% | Time: 00:02:41\n",
      "Batch: 52 | Sended: 95 | Retrieved: 95; Translated: 100.0% | Time: 00:02:14\n",
      "Batch: 53 | Sended: 89 | Retrieved: 81; Translated: 100.0% | Time: 00:02:39\n",
      "Batch: 54 | Sended: 96 | Retrieved: 93; Translated: 99.0% | Time: 00:02:36\n",
      "Batch: 55 | Sended: 91 | Retrieved: 86; Translated: 100.0% | Time: 00:02:32\n",
      "Batch: 56 | Sended: 95 | Retrieved: 94; Translated: 100.0% | Time: 00:02:30\n",
      "Batch: 57 | Sended: 100 | Retrieved: 93; Translated: 100.0% | Time: 00:02:26\n",
      "Batch: 58 | Sended: 93 | Retrieved: 92; Translated: 100.0% | Time: 00:02:24\n",
      "Batch: 59 | Sended: 86 | Retrieved: 86; Translated: 100.0% | Time: 00:02:36\n",
      "Batch: 60 | Sended: 88 | Retrieved: 86; Translated: 100.0% | Time: 00:01:53\n",
      "Batch: 61 | Sended: 85 | Retrieved: 84; Translated: 100.0% | Time: 00:01:43\n",
      "Batch: 62 | Sended: 86 | Retrieved: 86; Translated: 100.0% | Time: 00:02:39\n",
      "Batch: 63 | Sended: 93 | Retrieved: 87; Translated: 100.0% | Time: 00:02:28\n",
      "Batch: 64 | Sended: 87 | Retrieved: 87; Translated: 100.0% | Time: 00:02:19\n",
      "Batch: 65 | Sended: 90 | Retrieved: 85; Translated: 100.0% | Time: 00:02:40\n",
      "Batch: 66 | Sended: 99 | Retrieved: 93; Translated: 100.0% | Time: 00:02:21\n",
      "Batch: 67 | Sended: 103 | Retrieved: 95; Translated: 100.0% | Time: 00:01:51\n",
      "Batch: 68 | Sended: 95 | Retrieved: 92; Translated: 100.0% | Time: 00:02:08\n",
      "Batch: 69 | Sended: 97 | Retrieved: 92; Translated: 99.0% | Time: 00:01:54\n",
      "Batch: 70 | Sended: 94 | Retrieved: 91; Translated: 100.0% | Time: 00:02:15\n",
      "Batch: 71 | Sended: 97 | Retrieved: 91; Translated: 99.0% | Time: 00:02:18\n",
      "Batch: 72 | Sended: 87 | Retrieved: 86; Translated: 100.0% | Time: 00:02:22\n",
      "Batch: 73 | Sended: 91 | Retrieved: 91; Translated: 100.0% | Time: 00:02:18\n",
      "Batch: 74 | Sended: 97 | Retrieved: 96; Translated: 100.0% | Time: 00:02:13\n",
      "Batch: 75 | Sended: 88 | Retrieved: 88; Translated: 100.0% | Time: 00:01:49\n",
      "Batch: 76 | Sended: 96 | Retrieved: 93; Translated: 100.0% | Time: 00:02:15\n",
      "Batch: 77 | Sended: 94 | Retrieved: 92; Translated: 100.0% | Time: 00:02:17\n",
      "Batch: 78 | Sended: 88 | Retrieved: 53; Translated: 100.0% | Time: 00:01:24\n",
      "Batch: 79 | Sended: 88 | Retrieved: 86; Translated: 100.0% | Time: 00:02:10\n",
      "Batch: 80 | Sended: 85 | Retrieved: 84; Translated: 100.0% | Time: 00:02:10\n",
      "Batch: 81 | Sended: 85 | Retrieved: 83; Translated: 100.0% | Time: 00:02:22\n",
      "Batch: 82 | Sended: 90 | Retrieved: 89; Translated: 100.0% | Time: 00:02:19\n",
      "Batch: 83 | Sended: 88 | Retrieved: 84; Translated: 100.0% | Time: 00:02:21\n",
      "Batch: 84 | Sended: 84 | Retrieved: 84; Translated: 100.0% | Time: 00:02:16\n",
      "Batch: 85 | Sended: 93 | Retrieved: 90; Translated: 100.0% | Time: 00:02:18\n",
      "Batch: 86 | Sended: 94 | Retrieved: 94; Translated: 100.0% | Time: 00:02:20\n",
      "Batch: 87 | Sended: 83 | Retrieved: 83; Translated: 100.0% | Time: 00:02:20\n",
      "Batch: 88 | Sended: 90 | Retrieved: 88; Translated: 100.0% | Time: 00:02:22\n",
      "Batch: 89 | Sended: 87 | Retrieved: 85; Translated: 100.0% | Time: 00:02:17\n",
      "Batch: 90 | Sended: 87 | Retrieved: 87; Translated: 100.0% | Time: 00:02:18\n",
      "Batch: 91 | Sended: 92 | Retrieved: 92; Translated: 99.0% | Time: 00:02:48\n",
      "Batch: 92 | Sended: 89 | Retrieved: 87; Translated: 100.0% | Time: 00:02:25\n",
      "Batch: 93 | Sended: 85 | Retrieved: 85; Translated: 100.0% | Time: 00:01:51\n",
      "Batch: 94 | Sended: 91 | Retrieved: 71; Translated: 100.0% | Time: 00:01:48\n",
      "Batch: 95 | Sended: 97 | Retrieved: 95; Translated: 100.0% | Time: 00:02:07\n",
      "Batch: 96 | Sended: 92 | Retrieved: 91; Translated: 100.0% | Time: 00:02:15\n",
      "Batch: 97 | Sended: 89 | Retrieved: 88; Translated: 100.0% | Time: 00:02:20\n",
      "Batch: 98 | Sended: 87 | Retrieved: 86; Translated: 100.0% | Time: 00:02:17\n",
      "Batch: 99 | Sended: 86 | Retrieved: 86; Translated: 100.0% | Time: 00:02:17\n",
      "Batch: 100 | Sended: 92 | Retrieved: 90; Translated: 100.0% | Time: 00:02:19\n",
      "Batch: 101 | Sended: 91 | Retrieved: 92; Translated: 100.0% | Time: 00:02:24\n",
      "Batch: 102 | Sended: 89 | Retrieved: 87; Translated: 100.0% | Time: 00:02:37\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "translated_text, _ = translate(cleaned_data_en.text[630:], original=\"English\", new=\"Mexican Spanish\", verbose=True)\n",
    "t1 = time.time()\n",
    "print(f\"\\nAll tweets translated in: {prettytime(t1-t0)} hh:mm:ss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
