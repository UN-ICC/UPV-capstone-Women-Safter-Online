{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4065809-62c9-48af-b408-f4854fd1e93d",
   "metadata": {},
   "source": [
    "# Sexism Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351e9106-0564-4e29-8623-5394b274b52c",
   "metadata": {},
   "source": [
    "In any Data Science project, the first step is to obtain a dataset to work with. In the case of VAWG, data has been collected from various sources to acquire a considerable number of texts labeled as either sexist or non-sexist.\n",
    "\n",
    "This Notebook outlines the processes that were followed to obtain the final dataset and presents the implemented functions that will be included in the `utils.data` module. Data preprocessing in this case consists of three phases:\n",
    "\n",
    "1. **Source Integration**: involves the homogenization and merging of all collected datasets to create a unified dataset.\n",
    "1. **Text Cleaning**: involves the removal of all characters that could negatively affect the model.\n",
    "1. **Specific Translation**: involves translating data from another language to Spanish. In our case, the data has been translated into Mexican Spanish using GPT-4 since the project focuses on Mexico. This decision was made because GPT-4 has the ability to adjust the text to sound more like Mexican Spanish, including more culturally specific expressions. Other translation tools such as DeepTL or Google Translate were not chosen because they do not have the same level of customization as GPT-4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e149c689-c5b8-4f05-b755-04d75306d305",
   "metadata": {},
   "source": [
    "**Requiered libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "60b07f5c-e34b-4450-864d-b66873795a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to handle text\n",
    "import re  # regexs\n",
    "import emoji  # emojis \n",
    "import string  # punctuation\n",
    "import unidecode  # accents\n",
    "\n",
    "# to manage data\n",
    "import collections  # additional data structures\n",
    "import pandas as pd  # efficient tabular data \n",
    "import numpy as np  # efficient arrays\n",
    "\n",
    "# misc\n",
    "from copy import deepcopy  # for copying deep dictionaries\n",
    "\n",
    "# support for type hints\n",
    "from typing import List, Dict, Tuple, Union, Any, Optional "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ceeff-df87-4720-b4c1-aa6e05f5a580",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Source Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca0685b-7ade-4dc8-9767-93713c467898",
   "metadata": {},
   "source": [
    "The first step is to merge all the datasets into one while taking into account the structural differences between them. For example, not all datasets have the same variable names, some have more variables, and others may lack variables that are of interest to us. It can be quite a mess.\n",
    "\n",
    "To carry out this integration, two functions have been defined:\n",
    "\n",
    "- `homogenize`: This function is used to convert a dataset's specific format (column names, variables, etc.) to a \"common\" format (same number of columns, order, names, etc.) so that they can be easily integrated by concatenating them.\n",
    "\n",
    "- `integrate`: This function concatenates the data and has been implemented to work with an \"integration schema.\" This schema allows for the reading, homogenization, and integration of all sources at once without having to call the functions manually for each dataset.\n",
    "\n",
    "To further streamline the process, these functions have been wrapped into an object called `SexismDataIntegrator`, where they are defined as methods. By defining these functions as methods, they can be accessed through a single object, making it easier to use and manage them. Additionally, this approach allows for better encapsulation of the data integration process, making it easier to maintain and modify in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c11774a0-6cfb-41eb-9f67-7fc5ff68ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = lambda x, positive: 1 if x == positive else 0\n",
    "int2label = lambda x, positive, negative: positive if x else negative\n",
    "\n",
    "\n",
    "class SexismDataIntegrator:\n",
    "    def __init__(self, \n",
    "            common_columns: collections.OrderedDict,\n",
    "            original_data_path: str,\n",
    "            transformed_data_path: str\n",
    "        ):\n",
    "        \"\"\"\n",
    "        PARAMETERS:\n",
    "            - common_columns: must contain the key \"target\".\n",
    "            - original_data_path: ...\n",
    "            - transformed_data_path: ...\n",
    "        \"\"\"\n",
    "        self.common = common_columns\n",
    "        self.orig_data_path = original_data_path\n",
    "        self.trans_data_path = transformed_data_path\n",
    "        \n",
    "    def homogenize(self,\n",
    "                   \n",
    "            *datasets: List[pd.DataFrame],\n",
    "            column_mapping: Dict[Union[str, None], Union[str, list]] = {},\n",
    "            label_positive: Optional[Union[bool, str]] = None,\n",
    "            target_column: Optional[str] = None,\n",
    "            language: Optional[str] = None,\n",
    "            dataset_name: Optional[str] = None\n",
    "                   \n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Set a common format for any data source. Data returned will have the columns: specified in self.COMMON plus \n",
    "        \"dataset\" (if required). It also supports valuing the language column if present in self.COMMON and not in\n",
    "        original data.  \n",
    "\n",
    "        PARAMETERS: \n",
    "            - datasets: list of data frames to homogenize (same format expected between them). Starred expression\n",
    "              allow to pass directly several pandas.DataFrame objects.\n",
    "            - column_mapping: dictionary containing how to map actual to required columns if different names. It may\n",
    "              contain the key \"None\", which must map with a list containing the required common columns not present\n",
    "              in the data source. \n",
    "            - label_positive: the positive label to consider when astyping target column to int (if needed, \n",
    "              i.e. target column is not int).\n",
    "            - dataset_name: name to identify the data source.\n",
    "            - language: data source text's language (if not present in column_mapping).\n",
    "\n",
    "        RETURNS: homogenized data.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(datasets) == 1:\n",
    "            aux = datasets[0].copy()\n",
    "        else: \n",
    "            aux = pd.concat(datasets, ignore_index=True)\n",
    "                    \n",
    "        # some relevant variables\n",
    "        len_df = len(aux)\n",
    "        cmap = deepcopy(column_mapping)\n",
    "        \n",
    "        # add required columns not present in the source\n",
    "        none_columns = cmap.get(None, 0)\n",
    "        if none_columns:\n",
    "            for column in none_columns: aux[column] = [None] * len_df \n",
    "            del cmap[None]\n",
    "            \n",
    "        # renaming columns to common names\n",
    "        aux = aux.rename(columns=cmap) \n",
    "\n",
    "        # astype target column to int if needed\n",
    "        if label_positive is not None:\n",
    "\n",
    "            # if its boolean, astyping directly more efficient\n",
    "            if isinstance(label_positive, bool): \n",
    "                aux[self.common[\"target\"]] = aux[self.common[\"target\"]].astype(int)\n",
    "\n",
    "            # if its string, conventional label to int\n",
    "            elif isinstance(label_positive, str): \n",
    "                aux[self.common[\"target\"]] = aux[self.common[\"target\"]].apply(label2int, positive=label_positive)\n",
    "                \n",
    "        # add language column\n",
    "        if language is not None:\n",
    "            aux['language'] = [language] * len_df\n",
    "        \n",
    "        # select common columns\n",
    "        relevant_columns = list(self.common.values()) \n",
    "        aux = aux.loc[:, relevant_columns]\n",
    "\n",
    "        # add dataset name column \n",
    "        if dataset_name is not None:\n",
    "            aux['dataset'] = [dataset_name] * len_df\n",
    "\n",
    "        return aux\n",
    "    \n",
    "    def integration_schema_template(self):\n",
    "        print(\"\"\"\n",
    "schema = {\n",
    " <data_source_folder_name_in_original_data_path>: {\n",
    "     \"read\": {\n",
    "         \"files\" : [\n",
    "             <relative_filename1>,\n",
    "             <relative_filename2>,\n",
    "             ...\n",
    "         ],\n",
    "         \"kwargs\": {\n",
    "             <pandas_read_csv_kwarg1>: value_kwarg1,\n",
    "             <pandas_read_csv_kwarg2>: value_kwarg2,\n",
    "             ...\n",
    "         } (if needed)                   \n",
    "     },\n",
    "     \"homogenize\": {\n",
    "         \"column_mapping\": {\n",
    "             <actual_column_name1_in_data_source>: <required_common_column_name1>, (if different)\n",
    "             <actual_column_name2_in_data_source>: <required_common_column_name2>, (if different)\n",
    "             ... \n",
    "             None: [\n",
    "                 <required_common_column1_not_in_source>, \n",
    "                 <required_common_column2_not_in_source>,\n",
    "                 ...\n",
    "             ] (if needed)\n",
    "         },\n",
    "         \"label_positive\": value (if needed)\n",
    "         \"language\": value (if needed)\n",
    "     }\n",
    " },\n",
    " ...\n",
    "}\n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "    IntegrationSchemaType = Dict[str, Dict[str, Dict[str, Union[str, bool, List[str], Dict[str, str]]]]]\n",
    "    def integrate(self, schema: IntegrationSchemaType, save: bool = False ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Homogenize and integrate all the data sources specified in the integration schema\n",
    "\n",
    "        PARAMETERS:\n",
    "            - schema: Each key (data source folder name containing different files (train, test, all...)) \n",
    "              maps to both parameter sets (also dicts) required for read and homogenize properly. \n",
    "\n",
    "              The first dict (key \"read\") corresponds to the ones used by the function `pandas.read_csv` when reading \n",
    "              each file specified inside the folder. This dict must contain a list with each file name on the \n",
    "              key \"files\". It may also include another key (\"kwargs\") including other paramters needed when reading, \n",
    "              but only when necessary. The second dict (key \"homogenize\") corresponds to the keyword args used by \n",
    "              `SexismDataIntegrator.homogenize` and should contain \"column_mapping\" and \"label_positive\". Data source \n",
    "              folder name will be used for its `dataset_name_` parameter. For further understandig, please refer to \n",
    "              `SexismDataIntegrator.homogenize` documentation.\n",
    "\n",
    "              If one of the sources is multilingual, files are separated by language and there is not any column \n",
    "              containing such information, this source should appear as many times in the schema as languages are, \n",
    "              and each entry should by something like \"<data_source_folder_name>_<lang>\".\n",
    "\n",
    "              To better understand the concept, an schema of the schema can be found calling the defined method\n",
    "              `SexismDataIntegrator.integration_schema_template`.\n",
    "\n",
    "              - save: True for saving it to the self.trans_data_path\n",
    "\n",
    "        RETURNS: integrated homogenized data\n",
    "        \"\"\"\n",
    "        # buffer to store data sources homogenized\n",
    "        buffer = [] \n",
    "\n",
    "        # for each data source\n",
    "        for dataset_name_, kwargs in schema.items():\n",
    "\n",
    "            # prevent different naming when multilingual splitted cases\n",
    "            folder_name = dataset_name_.split(\"_\")[0]\n",
    "\n",
    "            # read data files contained in each data source\n",
    "            folder_path = f\"{self.orig_data_path}/{folder_name}/\"\n",
    "            kwargs_read = kwargs[\"read\"].get(\"kwargs\", {})\n",
    "            datasets = [ pd.read_csv(folder_path+file, **kwargs_read) for file in kwargs[\"read\"][\"files\"] ]\n",
    "\n",
    "            # homogenize data source\n",
    "            homogenized = self.homogenize(*datasets, **kwargs['homogenize'], dataset_name=folder_name)\n",
    "            buffer.append(homogenized)\n",
    "\n",
    "        # concat all homogenized data sources stored at the buffer\n",
    "        integrated = pd.concat(buffer, axis=0).reset_index(drop=True)\n",
    "        \n",
    "        # save reults\n",
    "        if save:\n",
    "            path = f\"{self.trans_data_path}/integrated_data.csv\"\n",
    "            integrated.to_csv(path, sep=\";\")\n",
    "            print(f\"Data correctly integrated and saved in \\033[1m'{path}'\\033[0m\")\n",
    "        \n",
    "        return integrated "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cff9a55-b2db-44d5-b436-a93af606ad21",
   "metadata": {},
   "source": [
    "**Custom Specification**\n",
    "\n",
    "The next step is to define the variables required for data integration according to our needs and create the `SexismDataIntegrator` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d7c7d659-6f22-4951-af1c-2a4b312e4237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contiene las carpetas de cada fuente de datos con sus respectivos ficheros\n",
    "ORIGINAL_DATA_PATH = \"../data/original\"\n",
    "\n",
    "# servirá para guardar los nuevos conjuntos de datos derivados de los anteriores\n",
    "TRANSFORMED_DATA_PATH = \"../data/transformed\"\n",
    "\n",
    "# definimos las columnas que queremos que tenga el conjunto de datos final\n",
    "# las claves son un identificador para esa columna y los valores el nombre que aparecerá\n",
    "# en el data frame. Debe contener la clave \"target\", donde se especifica la columna\n",
    "# que se necesita homogeneizar (pasar a numérico).\n",
    "COMMON_COLUMNS = collections.OrderedDict([(\"id\",\"original_id\"), \n",
    "                                          (\"txt\",\"text\"), \n",
    "                                          (\"target\",\"label\"), \n",
    "                                          (\"typ\", \"type\"),\n",
    "                                          (\"lang\", \"language\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "29d1cc5e-5d71-4c21-9ee8-b96bba45e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdi = SexismDataIntegrator(COMMON_COLUMNS, ORIGINAL_DATA_PATH, TRANSFORMED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb73d9-d157-475f-bf1a-40bbeb928b16",
   "metadata": {},
   "source": [
    "To define the integration schema, all we need to do is follow the template for each data source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "63384eee-f91f-4d63-a2d1-4e06bac0a95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "schema = {\n",
      " <data_source_folder_name_in_original_data_path>: {\n",
      "     \"read\": {\n",
      "         \"files\" : [\n",
      "             <relative_filename1>,\n",
      "             <relative_filename2>,\n",
      "             ...\n",
      "         ],\n",
      "         \"kwargs\": {\n",
      "             <pandas_read_csv_kwarg1>: value_kwarg1,\n",
      "             <pandas_read_csv_kwarg2>: value_kwarg2,\n",
      "             ...\n",
      "         } (if needed)                   \n",
      "     },\n",
      "     \"homogenize\": {\n",
      "         \"column_mapping\": {\n",
      "             <actual_column_name1_in_data_source>: <required_common_column_name1>, (if different)\n",
      "             <actual_column_name2_in_data_source>: <required_common_column_name2>, (if different)\n",
      "             ... \n",
      "             None: [\n",
      "                 <required_common_column1_not_in_source>, \n",
      "                 <required_common_column2_not_in_source>,\n",
      "                 ...\n",
      "             ] (if needed)\n",
      "         },\n",
      "         \"label_positive\": value (if needed)\n",
      "         \"language\": value (if needed)\n",
      "     }\n",
      " },\n",
      " ...\n",
      "}\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "sdi.integration_schema_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fc08deb0-1a3f-4010-865b-91ca95f5642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTEGRATION_SCHEMA = {\n",
    "    \n",
    "    \"callme\": {\n",
    "        \"read\": {\n",
    "            \"files\": [\n",
    "                \"sexism_data.csv\"\n",
    "            ]\n",
    "        },\n",
    "        \"homogenize\": {\n",
    "            \"column_mapping\": {\n",
    "                \"id\": COMMON_COLUMNS[\"id\"],  \n",
    "                \"sexist\": COMMON_COLUMNS[\"target\"],\n",
    "                None: [\n",
    "                    COMMON_COLUMNS[\"typ\"]\n",
    "                ]\n",
    "            },\n",
    "            \"label_positive\": True,\n",
    "            \"language\": \"en\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"edos\": {\n",
    "        \"read\": {\n",
    "            \"files\": [\n",
    "                \"edos_labelled_aggregated.csv\"\n",
    "            ]\n",
    "        },\n",
    "        \"homogenize\": {\n",
    "            \"column_mapping\": {\n",
    "                \"rewire_id\": COMMON_COLUMNS[\"id\"],\n",
    "                \"label_sexist\": COMMON_COLUMNS[\"target\"],\n",
    "                \"label_category\": COMMON_COLUMNS[\"typ\"],\n",
    "            },\n",
    "            \"label_positive\": \"sexist\",\n",
    "            \"language\": \"en\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"evalita\": {\n",
    "        \"read\": {\n",
    "            \"files\": [\n",
    "                \"en_training.tsv\", \n",
    "                \"en_testing.tsv\"\n",
    "            ],\n",
    "            \"kwargs\": {\n",
    "                \"sep\": \"\\t\"\n",
    "            }\n",
    "        },\n",
    "        \"homogenize\": {\n",
    "            \"column_mapping\": {\n",
    "                \"id\": COMMON_COLUMNS[\"id\"], \n",
    "                \"misogynous\": COMMON_COLUMNS[\"target\"],\n",
    "                \"misogyny_category\": COMMON_COLUMNS[\"typ\"]\n",
    "            },\n",
    "            \"language\": \"en\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"exist\": {\n",
    "        \"read\": {\n",
    "            \"files\": [\n",
    "                \"EXIST2021_training.tsv\", \n",
    "                \"EXIST2021_test_labeled.tsv\"\n",
    "            ],\n",
    "            \"kwargs\": {\n",
    "                \"sep\": \"\\t\"\n",
    "            }\n",
    "        },\n",
    "        \"homogenize\": {\n",
    "            \"column_mapping\": {\n",
    "                \"id\": COMMON_COLUMNS[\"id\"], \n",
    "                \"task1\": COMMON_COLUMNS[\"target\"],\n",
    "                \"task2\": COMMON_COLUMNS[\"typ\"],\n",
    "                \"language\": COMMON_COLUMNS[\"lang\"]\n",
    "            },\n",
    "            \"label_positive\": \"sexist\",\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"ibereval_en\": {\n",
    "        \"read\": {\n",
    "            \"files\": [\n",
    "                \"en_AMI_TrainingSet_NEW.csv\"\n",
    "            ],\n",
    "            \"kwargs\": {\n",
    "                \"sep\": \";\"\n",
    "            }\n",
    "        },\n",
    "        \"homogenize\": {\n",
    "            \"column_mapping\": {\n",
    "                \"id\": COMMON_COLUMNS[\"id\"], \n",
    "                \"tweet\": COMMON_COLUMNS[\"txt\"], \n",
    "                \"misogynous\": COMMON_COLUMNS[\"target\"],\n",
    "                \"misogyny_category\": COMMON_COLUMNS[\"typ\"]\n",
    "            },\n",
    "            \"language\": \"en\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"ibereval_es\": {\n",
    "        \"read\": {\n",
    "            \"files\": [\n",
    "                \"es_AMI_TrainingSet_NEW.csv\"\n",
    "            ],\n",
    "            \"kwargs\": {\n",
    "                \"sep\": \";\"\n",
    "            }\n",
    "        },\n",
    "        \"homogenize\": {\n",
    "            \"column_mapping\": {\n",
    "                \"id\": COMMON_COLUMNS[\"id\"], \n",
    "                \"tweet\": COMMON_COLUMNS[\"txt\"], \n",
    "                \"misogynous\": COMMON_COLUMNS[\"target\"],\n",
    "                \"misogyny_category\": COMMON_COLUMNS[\"typ\"]\n",
    "            },\n",
    "            \"language\": \"es\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"metwo\": {\n",
    "        \"read\": {\n",
    "            \"files\": [\n",
    "                \"targetResultFile_full2.csv\"\n",
    "            ],\n",
    "            \"kwargs\": {\n",
    "                \"sep\": \";\", \n",
    "                \"names\": [\n",
    "                    COMMON_COLUMNS[\"id\"], \n",
    "                    COMMON_COLUMNS[\"txt\"], \n",
    "                    COMMON_COLUMNS[\"target\"]\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"homogenize\": {\n",
    "            \"column_mapping\": {\n",
    "                None: [\n",
    "                    COMMON_COLUMNS[\"typ\"]\n",
    "                ]\n",
    "            },\n",
    "            \"label_positive\": \"SEXIST\",\n",
    "            \"language\": \"es\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1dc22d-0eba-4635-a690-e837b7c5e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "hateval_integration_schema = {\n",
    "    \n",
    "    \"hateval_en\": {\n",
    "        \"read\": {\n",
    "            \"files\": [\n",
    "                \"hateval2019_en_dev.csv\", \n",
    "                \"hateval2019_en_test.csv\", \n",
    "                \"hateval2019_en_train.csv\"\n",
    "            ]\n",
    "        },\n",
    "        \"homogenize\": {\n",
    "            \"column_mapping\": {\n",
    "                \"id\": COMMON_COLUMNS[\"id\"], \n",
    "                \"text\": COMMON_COLUMNS[\"text\"],\n",
    "                \"HS\": COMMON_COLUMNS[\"target\"],\n",
    "                None: [\n",
    "                    COMMON_COLUMNS[\"typ\"]\n",
    "                ]\n",
    "            },\n",
    "            \"language\": \"en\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"hateval_es\": {\n",
    "        \"read\": {\n",
    "            \"files\": [\n",
    "                \"hateval2019_es_dev.csv\", \n",
    "                \"hateval2019_es_test.csv\", \n",
    "                \"hateval2019_es_train.csv\"\n",
    "            ]\n",
    "        },\n",
    "        \"homogenize\": {\n",
    "            \"column_mapping\": {\n",
    "                \"id\": COMMON_COLUMNS[\"id\"], \n",
    "                \"text\": COMMON_COLUMNS[\"text\"],\n",
    "                \"HS\": COMMON_COLUMNS[\"target\"],\n",
    "                None: [\n",
    "                    COMMON_COLUMNS[\"typ\"]\n",
    "                ]\n",
    "            },\n",
    "            \"language\": \"es\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "INTEGRATION_SCHEMA.update(hateval_integration_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eea564c-5aff-4ce9-b4d5-543a1e994f93",
   "metadata": {},
   "source": [
    "With the integration schema properly defined, all that's left is to call the `integrate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2d00b709-5187-42df-b993-736bc9eece98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data correctly integrated and saved in \u001b[1m'../data/transformed/integrated_data.csv'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MENTION3481 i didn't even know random was an o...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Bottom two should've gone!  #mkr</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MENTION3111 MENTION3424 ladyboner deserves so ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>She shall now be known as Sourpuss #MKR #KatAn...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Tarah W threw a bunch of women under the bus s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59000</th>\n",
       "      <td>1047687262455177217</td>\n",
       "      <td>Yo no puedo darte luz todos los días, pero si ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59001</th>\n",
       "      <td>1064482731739045888</td>\n",
       "      <td>Que bien! Aunque digan que las mujeres no debe...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59002</th>\n",
       "      <td>1040584804536856577</td>\n",
       "      <td>@AriOrsingher Y misoginia las pelotas no quier...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59003</th>\n",
       "      <td>1051458429280235520</td>\n",
       "      <td>\"Imaginen el tipo de sociedad mojigata y castr...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59004</th>\n",
       "      <td>1020717495605489673</td>\n",
       "      <td>@SenadoraBlas por favor pase a la historia com...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59005 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               original_id                                               text  \\\n",
       "0                        0  MENTION3481 i didn't even know random was an o...   \n",
       "1                        1                   Bottom two should've gone!  #mkr   \n",
       "2                        2  MENTION3111 MENTION3424 ladyboner deserves so ...   \n",
       "3                        3  She shall now be known as Sourpuss #MKR #KatAn...   \n",
       "4                        4  Tarah W threw a bunch of women under the bus s...   \n",
       "...                    ...                                                ...   \n",
       "59000  1047687262455177217  Yo no puedo darte luz todos los días, pero si ...   \n",
       "59001  1064482731739045888  Que bien! Aunque digan que las mujeres no debe...   \n",
       "59002  1040584804536856577  @AriOrsingher Y misoginia las pelotas no quier...   \n",
       "59003  1051458429280235520  \"Imaginen el tipo de sociedad mojigata y castr...   \n",
       "59004  1020717495605489673  @SenadoraBlas por favor pase a la historia com...   \n",
       "\n",
       "       label  type language dataset  \n",
       "0          0  None       en  callme  \n",
       "1          0  None       en  callme  \n",
       "2          0  None       en  callme  \n",
       "3          0  None       en  callme  \n",
       "4          0  None       en  callme  \n",
       "...      ...   ...      ...     ...  \n",
       "59000      0  None       es   metwo  \n",
       "59001      0  None       es   metwo  \n",
       "59002      0  None       es   metwo  \n",
       "59003      0  None       es   metwo  \n",
       "59004      0  None       es   metwo  \n",
       "\n",
       "[59005 rows x 6 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integrated = sdi.integrate(INTEGRATION_SCHEMA, save=True)\n",
    "integrated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc97560-e3f7-44ce-a0bf-a47afcd3456d",
   "metadata": {},
   "source": [
    "And some relevant proportions and counts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "45a1f2af-9ee8-4fd4-a9d1-acf5d65426b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-sexist: 68.53%\n",
      "Sexist: 31.47%\n"
     ]
    }
   ],
   "source": [
    "count = integrated_data.label.sum()\n",
    "total = len(integrated_data)\n",
    "print(f\"Non-sexist: {round((total-count)/total*100, 2)}%\\nSexist: {round(count/total*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f223ce8-bb3a-4449-b416-1589256568b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: 80.55%\n",
      "Spanish: 19.45%\n"
     ]
    }
   ],
   "source": [
    "count = (integrated_data.language == \"en\").sum()\n",
    "print(f\"English: {round(count/total*100, 2)}%\\nSpanish: {round((total-count)/total*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "965514d7-3372-4731-9e69-e00da7a999c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>language</th>\n",
       "      <th>English</th>\n",
       "      <th>Spanish</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Non-Sexist</th>\n",
       "      <td>34256</td>\n",
       "      <td>6182</td>\n",
       "      <td>40438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sexist</th>\n",
       "      <td>13270</td>\n",
       "      <td>5297</td>\n",
       "      <td>18567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>47526</td>\n",
       "      <td>11479</td>\n",
       "      <td>59005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "language    English  Spanish    All\n",
       "label                              \n",
       "Non-Sexist    34256     6182  40438\n",
       "Sexist        13270     5297  18567\n",
       "All           47526    11479  59005"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(\n",
    "    integrated_data.label.apply(int2label, positive=\"Sexist\", negative=\"Non-Sexist\"),\n",
    "    integrated_data.language.apply(lambda x: \"English\" if x == \"en\" else \"Spanish\"),\n",
    "    margins = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4646e587-dae4-4908-bf66-100ff2793c5f",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f339e53-eb72-44c7-a0bd-ee17583ad12f",
   "metadata": {},
   "source": [
    "Next step in our pre-process is cleaning text properly. For this task function `clean` is defined, which handles case, blanks, numbers, links, mentions, hashtags, emojis, accents, symbols and punctuation. It allows customizing links, mentions, hashtag and emojis handling and adding custom regexs and its substitution in cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bf28c248-e250-4c1c-88ab-99dc16f62f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(\n",
    "    \n",
    "        text: str, \n",
    "        keep_case: bool = False,\n",
    "        keep_accents: bool = False,\n",
    "        keep_numbers: bool = False,\n",
    "        lmhe_tokens: Optional[Dict[str, str]] = None, \n",
    "        constraints: Optional[List[Tuple[str, str]]] = None,\n",
    "        allowed_punctuation: Optional[str] = None\n",
    "    \n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Clean a given text\n",
    "    \n",
    "    PARAMETERS:\n",
    "        - text: string to clean\n",
    "        - keep_case: wether to keep original case (True) or not (False). If not text will be lowercased.\n",
    "        - keep_accents: wether to keep accents (True) or not (False).\n",
    "        - keep_numbers: wether to keep numbers (True) or not (False).\n",
    "        - lmhe_tokens: which stands for link-mention-hashtag-emoji_tokens. A dict containing how to represent.\n",
    "          those items in the final text. If nothing provided (neither dict or specific key), they will be removed.\n",
    "        - constraints: any special substitution you may want to apply to the text. It must be a list of tuples \n",
    "          containing the corresponding regex to capture (first element of the tuple) and the string to substitue \n",
    "          it (second element).\n",
    "        - allowed_punctuation: string containing custom punctuation you may want to avoid cleaning.\n",
    "        \n",
    "    RETURNS cleaned text\n",
    "    \"\"\"\n",
    "    \n",
    "    # lowercase\n",
    "    if not keep_case:\n",
    "        text = text.lower() \n",
    "    \n",
    "    #remove \\n and \\r\n",
    "    text = text.replace('\\r', '').replace('\\n', ' ')\n",
    "    \n",
    "    if lmhe_tokens is not None:\n",
    "        # handle links\n",
    "        text = re.sub(r'(?:www\\.|https?://)\\S+', lmhe_tokens.get(\"link\", ''), text, flags=re.MULTILINE)  \n",
    "        \n",
    "        # handle mentions\n",
    "        text = re.sub(r'\\@\\S+', lmhe_tokens.get(\"mention\", ''), text) \n",
    "        \n",
    "        # handle hashtags\n",
    "        text = re.sub(r'#\\S+', lmhe_tokens.get(\"hashtag\", ''), text)\n",
    "        \n",
    "        # handle emojis\n",
    "        text = emoji.replace_emoji(text, lmhe_tokens.get(\"emoji\", ''))  \n",
    "        \n",
    "    else:\n",
    "        # remove links, mentions, hashtags and emojis\n",
    "        text = re.sub(r'(?:#|\\@|www\\.|https?://)\\S+', '', text, flags=re.MULTILINE) \n",
    "        text = emoji.replace_emoji(text, '')\n",
    "     \n",
    "    # specific constraints\n",
    "    if constraints is not None:\n",
    "        for regex, token in constraints:\n",
    "            text = re.sub(regex, token, text, flags=re.I)\n",
    "    \n",
    "    # remove accents\n",
    "    if not keep_accents:\n",
    "        text = unidecode.unidecode(text)  \n",
    "    \n",
    "    ## all symbols and punctuation\n",
    "    banned_list = string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'+'—'  \n",
    "    ## allowed punctuation\n",
    "    if allowed_punctuation is not None:  \n",
    "        banned_list = re.sub(r\"[%s]\" % re.escape(allowed_punctuation), \"\", banned_list)\n",
    "    # remove symbols and punctuation\n",
    "    text = text.translate(str.maketrans('', '', banned_list)) \n",
    "    \n",
    "    # remove numbers\n",
    "    if not keep_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)  \n",
    "    \n",
    "    # remove extra and leading blanks\n",
    "    text = re.sub(\"\\s\\s+\" , \" \", text).strip()  \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca584bbd-a806-40cd-a115-43fb7a260db2",
   "metadata": {},
   "source": [
    "We want to maintain the following punctuation because the models we are going to use had been trained with almost raw text and well know how to interpret them. Also, exclamation and interrogation marks could express some emotions. We maintain inverted commas (') because we don't want cases such \"should've\" -> \"shouldve\" on English data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2f9ea78d-201e-405b-aa29-93d25f97d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_PUNCTUATION = \"'!¿?.,\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e97f40-87d5-4ac9-8df3-ac194b4a6557",
   "metadata": {},
   "source": [
    "We are going to maintain accents because of the spanish data and because BETO also handles them. Our constraints came from EDOS texts (links as \"\\[URL\\]\" and users as \"\\[USER\\]\") and CALLME texts (mentions as \"MENTION\\<number\\>\"), since we want to discard those information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a05ff13f-0fb5-44d8-ba1c-205c3eb7a4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>language</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i didn't even know random was an option!</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bottom two should've gone!</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ladyboner deserves so much more credit than du...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>she shall now be known as sourpuss</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tarah w threw a bunch of women under the bus s...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59000</th>\n",
       "      <td>1047687262455177217</td>\n",
       "      <td>yo no puedo darte luz todos los días, pero si ...</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59001</th>\n",
       "      <td>1064482731739045888</td>\n",
       "      <td>que bien! aunque digan que las mujeres no debe...</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59002</th>\n",
       "      <td>1040584804536856577</td>\n",
       "      <td>y misoginia las pelotas no quiero que vengas a...</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59003</th>\n",
       "      <td>1051458429280235520</td>\n",
       "      <td>imaginen el tipo de sociedad mojigata y castra...</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59004</th>\n",
       "      <td>1020717495605489673</td>\n",
       "      <td>por favor pase a la historia como una mujer qu...</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58775 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               original_id                                               text  \\\n",
       "0                        0           i didn't even know random was an option!   \n",
       "1                        1                         bottom two should've gone!   \n",
       "2                        2  ladyboner deserves so much more credit than du...   \n",
       "3                        3                 she shall now be known as sourpuss   \n",
       "4                        4  tarah w threw a bunch of women under the bus s...   \n",
       "...                    ...                                                ...   \n",
       "59000  1047687262455177217  yo no puedo darte luz todos los días, pero si ...   \n",
       "59001  1064482731739045888  que bien! aunque digan que las mujeres no debe...   \n",
       "59002  1040584804536856577  y misoginia las pelotas no quiero que vengas a...   \n",
       "59003  1051458429280235520  imaginen el tipo de sociedad mojigata y castra...   \n",
       "59004  1020717495605489673  por favor pase a la historia como una mujer qu...   \n",
       "\n",
       "       label language dataset  \n",
       "0          0       en  callme  \n",
       "1          0       en  callme  \n",
       "2          0       en  callme  \n",
       "3          0       en  callme  \n",
       "4          0       en  callme  \n",
       "...      ...      ...     ...  \n",
       "59000      0       es   metwo  \n",
       "59001      0       es   metwo  \n",
       "59002      0       es   metwo  \n",
       "59003      0       es   metwo  \n",
       "59004      0       es   metwo  \n",
       "\n",
       "[58775 rows x 5 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONSTRAINTS = [(r\"\\[URL\\]|\\[USER\\]\", \"\"), (r\"MENTION\\d+\", \"\")]\n",
    "cleaned_data = integrated_data.copy()\n",
    "cleaned_data.text = cleaned_data.text.apply(clean, \n",
    "                                            keep_case=False,\n",
    "                                            keep_accents=True,\n",
    "                                            keep_numbers=True,\n",
    "                                            constraints=CONSTRAINTS, \n",
    "                                            allowed_punctuation=ALLOWED_PUNCTUATION)\n",
    "\n",
    "#because of the transformations it is posible that some texts get completly empty\n",
    "cleaned_data = cleaned_data.loc[cleaned_data.text != \"\", :]\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7adb25d-5d62-4372-9fe5-bf67e71bcd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.to_csv(f\"{TRANSFORMED_DATA_PATH}/cleaned_data.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea8b3c1-837a-4963-a27b-9178bd25f79a",
   "metadata": {},
   "source": [
    "Another posibility could be to maintain links, mentions, hashtags and emojis as special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5712f2f0-136a-4e20-b928-112a7ddc30a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>language</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[USER] i didn't even know random was an option!</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bottom two should've gone! [HASHTAG]</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[USER] [USER] ladyboner deserves so much more ...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>she shall now be known as sourpuss [HASHTAG] [...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tarah w threw a bunch of women under the bus s...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59000</th>\n",
       "      <td>1047687262455177217</td>\n",
       "      <td>yo no puedo darte luz todos los días, pero si ...</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59001</th>\n",
       "      <td>1064482731739045888</td>\n",
       "      <td>que bien! aunque digan que las mujeres no debe...</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59002</th>\n",
       "      <td>1040584804536856577</td>\n",
       "      <td>[USER] y misoginia las pelotas no quiero que v...</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59003</th>\n",
       "      <td>1051458429280235520</td>\n",
       "      <td>imaginen el tipo de sociedad mojigata y castra...</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59004</th>\n",
       "      <td>1020717495605489673</td>\n",
       "      <td>[USER] por favor pase a la historia como una m...</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>metwo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59005 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               original_id                                               text  \\\n",
       "0                        0    [USER] i didn't even know random was an option!   \n",
       "1                        1               bottom two should've gone! [HASHTAG]   \n",
       "2                        2  [USER] [USER] ladyboner deserves so much more ...   \n",
       "3                        3  she shall now be known as sourpuss [HASHTAG] [...   \n",
       "4                        4  tarah w threw a bunch of women under the bus s...   \n",
       "...                    ...                                                ...   \n",
       "59000  1047687262455177217  yo no puedo darte luz todos los días, pero si ...   \n",
       "59001  1064482731739045888  que bien! aunque digan que las mujeres no debe...   \n",
       "59002  1040584804536856577  [USER] y misoginia las pelotas no quiero que v...   \n",
       "59003  1051458429280235520  imaginen el tipo de sociedad mojigata y castra...   \n",
       "59004  1020717495605489673  [USER] por favor pase a la historia como una m...   \n",
       "\n",
       "       label language dataset  \n",
       "0          0       en  callme  \n",
       "1          0       en  callme  \n",
       "2          0       en  callme  \n",
       "3          0       en  callme  \n",
       "4          0       en  callme  \n",
       "...      ...      ...     ...  \n",
       "59000      0       es   metwo  \n",
       "59001      0       es   metwo  \n",
       "59002      0       es   metwo  \n",
       "59003      0       es   metwo  \n",
       "59004      0       es   metwo  \n",
       "\n",
       "[59005 rows x 5 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONSTRAINTS = [(r\"MENTION\\d+\", \"[USER]\")]\n",
    "LMHE_TOKENS = {\n",
    "    \"link\": \"[URL]\", \n",
    "    \"mention\": \"[USER]\", \n",
    "    \"hashtag\": \"[HASHTAG]\", \n",
    "    \"emoji\": \"[EMOJI]\"\n",
    "}\n",
    "\n",
    "cleaned_data2 = integrated_data.copy()\n",
    "cleaned_data2.text = cleaned_data2.text.apply(clean, \n",
    "                                              keep_case=False,\n",
    "                                              keep_accents=True,\n",
    "                                              keep_numbers=True,\n",
    "                                              constraints=CONSTRAINTS,  \n",
    "                                              lmhe_tokens=LMHE_TOKENS, \n",
    "                                              allowed_punctuation=ALLOWED_PUNCTUATION+\"[]\")\n",
    "cleaned_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c0efb12-78ac-4c25-bd17-a7fda1fe367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data2.to_csv(f\"{TRANSFORMED_DATA_PATH}/prepared_data_with_tokens.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb753c9-9251-4b16-95db-6a146ab54e6d",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e796e1-0803-4931-b613-fb579e88380b",
   "metadata": {},
   "source": [
    "Still worwikg on it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "415c2c19-47ab-4197-a1b5-f6313bca2b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_id</th>\n",
       "      <th>text</th>\n",
       "      <th>flag</th>\n",
       "      <th>language</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i didn't even know random was an option!</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bottom two should've gone!</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ladyboner deserves so much more credit than du...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>she shall now be known as sourpuss</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tarah w threw a bunch of women under the bus s...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>callme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53222</th>\n",
       "      <td>3230</td>\n",
       "      <td>when someone announces they're unfollowing</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>ibereval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53223</th>\n",
       "      <td>3265</td>\n",
       "      <td>when someone announces that they are 'official...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>ibereval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53224</th>\n",
       "      <td>3387</td>\n",
       "      <td>deleted again. working to get it back again</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>ibereval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53225</th>\n",
       "      <td>3259</td>\n",
       "      <td>when you're on a first date and she asks to ta...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>ibereval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53226</th>\n",
       "      <td>3525</td>\n",
       "      <td>when your friend won't stop talking about thei...</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>ibereval</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47332 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      original_id                                               text  flag  \\\n",
       "0               0           i didn't even know random was an option!     0   \n",
       "1               1                         bottom two should've gone!     0   \n",
       "2               2  ladyboner deserves so much more credit than du...     0   \n",
       "3               3                 she shall now be known as sourpuss     0   \n",
       "4               4  tarah w threw a bunch of women under the bus s...     0   \n",
       "...           ...                                                ...   ...   \n",
       "53222        3230         when someone announces they're unfollowing     0   \n",
       "53223        3265  when someone announces that they are 'official...     0   \n",
       "53224        3387        deleted again. working to get it back again     0   \n",
       "53225        3259  when you're on a first date and she asks to ta...     0   \n",
       "53226        3525  when your friend won't stop talking about thei...     0   \n",
       "\n",
       "      language   dataset  \n",
       "0           en    callme  \n",
       "1           en    callme  \n",
       "2           en    callme  \n",
       "3           en    callme  \n",
       "4           en    callme  \n",
       "...        ...       ...  \n",
       "53222       en  ibereval  \n",
       "53223       en  ibereval  \n",
       "53224       en  ibereval  \n",
       "53225       en  ibereval  \n",
       "53226       en  ibereval  \n",
       "\n",
       "[47332 rows x 5 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data_en = cleaned_data.loc[cleaned_data.language == \"en\"]\n",
    "cleaned_data_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "da11a5a6-e058-4f2c-90ef-a7f1251f5ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_given_size(a, size):\n",
    "    return np.split(a, np.arange(size, len(a), size))\n",
    "\n",
    "def translate(texts: Union[pd.Series, list], from_: str, to_: str):\n",
    "    \n",
    "    translated = []\n",
    "    \n",
    "    # start conversation with gpt-4\n",
    "    \n",
    "    for batch in split_given_size(np.array(range(len(texts))), 50): \n",
    "        pass\n",
    "        \n",
    "        # generate prompt with batch\n",
    "    \n",
    "        # ask gpt-4\n",
    "        \n",
    "        # process response\n",
    "        \n",
    "        # accumulate translations\n",
    "    \n",
    "    return #transalted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e15654-53eb-4241-a905-086edbc32472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
